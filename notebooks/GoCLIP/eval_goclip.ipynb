{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80ad0d08-4a91-4574-900a-4d6ee2105557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Feature extraction for CLIP-GO (UNI2 + CoCa)\n",
    "===========================================\n",
    "\n",
    "• Reuses the training-time wiring (CFG, UNI2Wrapper, CLIPGO).\n",
    "• Loads the same AnnData + sentences CSV.\n",
    "• Builds a non-shuffled DataLoader.\n",
    "• Loads best/last checkpoint.\n",
    "• Saves NPZ with:\n",
    "    - cell_id\n",
    "    - vision_raw  : UNI2 pooled (4 center tokens), dim=1536\n",
    "    - vision_proj : projected + L2-normalized, dim=256\n",
    "    - text_raw    : CoCa encode_text output, dim≈768/1024 (depends on model)\n",
    "    - text_proj   : projected + L2-normalized, dim=256\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os, glob, math\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import timm\n",
    "import open_clip\n",
    "import scanpy as sc\n",
    "import openslide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77c8a8c1-ac3d-4e92-b07e-6ca225b6c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# CFG (match your training style)\n",
    "# -----------------------------------------------------------------------------\n",
    "class CFG:\n",
    "    # data\n",
    "    cancer = \"lung\"\n",
    "    ground_truth = \"refined\"\n",
    "    level = 0                 # UNI2 spatial token level (0 → 4-center tokens)\n",
    "    batch_size = 256\n",
    "    num_workers = 8\n",
    "\n",
    "    # embeddings / model dims\n",
    "    morph_emb_dims = 1536\n",
    "    projection_dim = 256\n",
    "    patch_size = 224\n",
    "\n",
    "    # Text / CoCa\n",
    "    coca_model = \"coca_ViT-L-14\"\n",
    "    coca_pretrain = \"laion2B-s13b-b90k\"\n",
    "    context_len = 76\n",
    "    freeze_text = True  # only matters if you change the model; extraction is no-grad\n",
    "\n",
    "    # paths (mirror your training)\n",
    "    root = \"/rsrch9/home/plm/idso_fa1_pathology/TIER1/paul-xenium/public_data/10x_genomics\"\n",
    "    xenium_sample_dict = {\n",
    "        \"lung\":\"Xenium_Prime_Human_Lung_Cancer_FFPE_outs\",\n",
    "        \"breast\": \"Xenium_Prime_Breast_Cancer_FFPE_outs\",\n",
    "        \"lymph_node\": \"Xenium_Prime_Human_Lymph_Node_Reactive_FFPE_outs\",\n",
    "        \"prostate\": \"Xenium_Prime_Human_Prostate_FFPE_outs\",\n",
    "        \"skin\": \"Xenium_Prime_Human_Skin_FFPE_outs\",\n",
    "        \"ovarian\": \"Xenium_Prime_Ovarian_Cancer_FFPE_outs\",\n",
    "        \"cervical\": \"Xenium_Prime_Cervical_Cancer_FFPE_outs\",\n",
    "    }\n",
    "    go_dir = \"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/gene_ontology\"\n",
    "    model_dir = \"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/models/public/UNI2-h\"\n",
    "    ckpt_dir  = \"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/models/fine_tuned/GoCLIP\"\n",
    "\n",
    "    target_mpp = 0.5  # target µm/px (≈20×)\n",
    "\n",
    "    # output\n",
    "    out_npz = \"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/models/fine_tuned/GoCLIP/features_lung.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92f1f73f-0d85-4b0b-909e-c304d847314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Projection head (same as training)\n",
    "# -----------------------------------------------------------------------------\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim: int, proj_dim: int = CFG.projection_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, proj_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(proj_dim, proj_dim)\n",
    "        self.ln  = nn.LayerNorm(proj_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.fc1(x)\n",
    "        x = self.act(h)\n",
    "        x = self.fc2(x)\n",
    "        return self.ln(x + h)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# UNI2 wrapper (4-centre token pooling; same as training)\n",
    "# -----------------------------------------------------------------------------\n",
    "class UNI2Wrapper(nn.Module):\n",
    "    def __init__(self, uni2: nn.Module, centre_idx: List[int]):\n",
    "        super().__init__()\n",
    "        self.uni2 = uni2\n",
    "        self.centre_idx = torch.tensor(centre_idx, dtype=torch.long)\n",
    "        self.prefix_tokens = 9\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        tok = self.uni2.forward_features(x)               # (B, 265, 1536)\n",
    "        spatial = tok[:, self.prefix_tokens :, :]\n",
    "        centre  = spatial.index_select(1, self.centre_idx.to(x.device)).mean(1)\n",
    "        return centre                                     # (B,1536)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Dataset: cell-centred patch + GO sentence (returns cell_id too)\n",
    "# -----------------------------------------------------------------------------\n",
    "class CellPatchTextDataset(Dataset):\n",
    "    def __init__(self, slide, cell_df: pd.DataFrame, sentences: pd.Series,\n",
    "                 transform, scale: float, patch_size: int = CFG.patch_size):\n",
    "        self.slide = slide\n",
    "        self.cells = cell_df.reset_index(drop=False)  # keep cell_id in column \"index\"\n",
    "        self.sentences = sentences\n",
    "        self.tfm = transform\n",
    "        self.scale = scale\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cells)\n",
    "\n",
    "    def _read_patch(self, x, y):\n",
    "        big = int(self.patch_size * self.scale)\n",
    "        tlx, tly = int(x - big/2), int(y - big/2)\n",
    "        patch = self.slide.read_region((tlx, tly), 0, (big, big)).convert(\"RGB\")\n",
    "        return patch.resize((self.patch_size, self.patch_size), Image.LANCZOS)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row   = self.cells.iloc[idx]\n",
    "        patch = self._read_patch(row.x_centroid, row.y_centroid)\n",
    "        img_t = self.tfm(patch)\n",
    "        cell_id = row[\"index\"]\n",
    "        sent = self.sentences.loc[cell_id]\n",
    "        sent = \"\" if pd.isna(sent) else str(sent)\n",
    "        return {\"image\": img_t, \"text\": sent, \"cell_id\": cell_id}\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CLIP-GO (UNI2 + CoCa) – same as training\n",
    "# -----------------------------------------------------------------------------\n",
    "class CLIPGO(nn.Module):\n",
    "    def __init__(self, vision_backbone: nn.Module,\n",
    "                 coca_model: str = CFG.coca_model,\n",
    "                 coca_pretrain: str = CFG.coca_pretrain,\n",
    "                 proj_dim: int = CFG.projection_dim,\n",
    "                 context_len: int = CFG.context_len,\n",
    "                 freeze_text: bool = CFG.freeze_text):\n",
    "        super().__init__()\n",
    "        self.context_len = context_len\n",
    "        self.freeze_text = freeze_text\n",
    "\n",
    "        # Vision branch\n",
    "        self.vision_encoder = vision_backbone\n",
    "        vision_dim = vision_backbone.uni2.embed_dim  # 1536 for ViT-Giant\n",
    "        self.vision_proj  = ProjectionHead(vision_dim, proj_dim)\n",
    "\n",
    "        # Text branch (CoCa causal)\n",
    "        self.text_encoder, _, _ = open_clip.create_model_and_transforms(\n",
    "            coca_model, pretrained=coca_pretrain,\n",
    "            cache_dir=os.path.expanduser(\"~/.cache/open_clip\")\n",
    "        )\n",
    "        self.tokenizer = open_clip.get_tokenizer(coca_model)\n",
    "        if freeze_text:\n",
    "            for p in self.text_encoder.parameters():\n",
    "                p.requires_grad_(False)\n",
    "\n",
    "        # Determine text width dynamically\n",
    "        with torch.no_grad():\n",
    "            dummy = self.tokenizer([\"dummy\"], context_length=context_len)\n",
    "            txt_feat = self.text_encoder.encode_text(dummy)\n",
    "        text_dim = txt_feat.shape[-1]\n",
    "        self.text_proj = ProjectionHead(text_dim, proj_dim)\n",
    "\n",
    "        # CLIP temperature (not used directly for extraction but kept for completeness)\n",
    "        self.logit_scale = nn.Parameter(torch.log(torch.tensor(1/0.07)))\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helpers: slide/MPP + transforms + checkpoint\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_slide_and_mpp(slide_dir: str):\n",
    "    tifs = sorted(glob.glob(os.path.join(slide_dir, \"**\", \"*he_image_registered*.ome.tif\"), recursive=True))\n",
    "    if not tifs:\n",
    "        tifs = sorted(glob.glob(os.path.join(slide_dir, \"**\", \"*.tif\"), recursive=True))\n",
    "        assert tifs, f\"No slide tif found under {slide_dir}\"\n",
    "    slide_path = tifs[0]\n",
    "    slide = openslide.open_slide(slide_path)\n",
    "\n",
    "    # mpp search (robust)\n",
    "    props = slide.properties\n",
    "    mpp = None\n",
    "    for key in (\"openslide.mpp-x\", \"aperio.MPP\", \"tiff.XResolution\"):\n",
    "        if key in props:\n",
    "            try:\n",
    "                mpp = float(props[key]); break\n",
    "            except Exception:\n",
    "                pass\n",
    "    if mpp is None:\n",
    "        mpp = CFG.target_mpp\n",
    "    return slide, mpp, slide_path\n",
    "\n",
    "\n",
    "def build_transforms():\n",
    "    return T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "    ])\n",
    "\n",
    "\n",
    "def resolve_checkpoint():\n",
    "    cands = [\n",
    "        os.path.join(CFG.ckpt_dir, \"best.pth\"),\n",
    "        os.path.join(CFG.ckpt_dir, CFG.cancer, \"best.pth\"),\n",
    "        os.path.join(CFG.ckpt_dir, \"last.pth\"),\n",
    "    ]\n",
    "    for p in cands:\n",
    "        if os.path.isfile(p):\n",
    "            return p\n",
    "    raise FileNotFoundError(f\"No checkpoint found in {CFG.ckpt_dir} (tried best.pth / last.pth).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6df11cdb-05b0-4a61-88c9-a68aba50329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Feature extraction\n",
    "# -----------------------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def extract_features(model: CLIPGO, loader: DataLoader, device: torch.device):\n",
    "    model.eval().to(device)\n",
    "    ids = []\n",
    "    vis_raw, vis_proj = [], []\n",
    "    txt_raw, txt_proj = [], []\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"Extracting features\", dynamic_ncols=True)\n",
    "    for batch in pbar:\n",
    "        imgs  = batch[\"image\"].to(device, non_blocking=True)\n",
    "        texts = batch[\"text\"]\n",
    "        batch_ids = batch[\"cell_id\"]\n",
    "        ids.extend(batch_ids)\n",
    "\n",
    "        # Vision\n",
    "        v_raw  = model.vision_encoder(imgs)                    # (B, 1536)\n",
    "        v_proj = F.normalize(model.vision_proj(v_raw), dim=-1) # (B, 256)\n",
    "\n",
    "        # Text\n",
    "        tokens = model.tokenizer(texts, context_length=model.context_len).to(device)\n",
    "        t_raw  = model.text_encoder.encode_text(tokens)        # (B, text_dim)\n",
    "        t_proj = F.normalize(model.text_proj(t_raw), dim=-1)   # (B, 256)\n",
    "\n",
    "        vis_raw.append(v_raw.cpu());   vis_proj.append(v_proj.cpu())\n",
    "        txt_raw.append(t_raw.cpu());   txt_proj.append(t_proj.cpu())\n",
    "\n",
    "    vis_raw  = torch.cat(vis_raw, 0).numpy()\n",
    "    vis_proj = torch.cat(vis_proj, 0).numpy()\n",
    "    txt_raw  = torch.cat(txt_raw, 0).numpy()\n",
    "    txt_proj = torch.cat(txt_proj, 0).numpy()\n",
    "    return ids, vis_raw, vis_proj, txt_raw, txt_proj\n",
    "\n",
    "\n",
    "def save_npz(path: str, cell_ids, vis_raw, vis_proj, txt_raw, txt_proj):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    np.savez_compressed(\n",
    "        path,\n",
    "        cell_id=np.array(cell_ids),\n",
    "        vision_raw=vis_raw,\n",
    "        vision_proj=vis_proj,\n",
    "        text_raw=txt_raw,\n",
    "        text_proj=txt_proj,\n",
    "    )\n",
    "    print(f\"✓ Saved → {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c9fa9f3-86e3-4bc9-9cac-2b0e4ee28cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Loading checkpoint: /rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/models/fine_tuned/GoCLIP/best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 956/956 [1:28:51<00:00,  5.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diag cosine (proj): mean=0.486, std=0.054\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Main\n",
    "# -----------------------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Resolve dataset paths\n",
    "sample  = CFG.xenium_sample_dict[CFG.cancer]\n",
    "sample_dir = os.path.join(CFG.root, sample)\n",
    "\n",
    "# Load AnnData (expects x_centroid / y_centroid in .obs)\n",
    "adata_path = os.path.join(\n",
    "    sample_dir,\n",
    "    \"preprocessed\",\n",
    "    f\"fine_tune_{CFG.ground_truth}_v2\",\n",
    "    f\"processed_xenium_data_fine_tune_{CFG.ground_truth}_v2_annotated.h5ad\",\n",
    ")\n",
    "adata = sc.read_h5ad(adata_path)\n",
    "cell_df = adata.obs.copy()  # index = cell_id\n",
    "\n",
    "# Sentences: same CSV logic as your training\n",
    "sent_path = f\"{CFG.go_dir}/{sample.replace('outs', 'GO.csv')}\"\n",
    "assert os.path.isfile(sent_path), f\"Missing sentences file: {sent_path}\"\n",
    "sentences = pd.read_csv(sent_path, index_col=\"cell_id\")[\"go_sentences\"].astype(str)\n",
    "sentences = sentences.reindex(cell_df.index).fillna(\"\")\n",
    "\n",
    "# Slide + scale\n",
    "slide, mpp, slide_path = get_slide_and_mpp(sample_dir)\n",
    "scale_factor = max(CFG.target_mpp / float(mpp), 1e-6)\n",
    "\n",
    "# Transforms and DataLoader (no shuffle, keep all)\n",
    "transform = build_transforms()\n",
    "dataset = CellPatchTextDataset(slide, cell_df, sentences, transform,\n",
    "                               scale=scale_factor, patch_size=CFG.patch_size)\n",
    "loader  = DataLoader(dataset, batch_size=CFG.batch_size, shuffle=False,\n",
    "                     num_workers=CFG.num_workers, pin_memory=True,\n",
    "                     persistent_workers=True)\n",
    "\n",
    "# Build vision backbone (UNI2) and wrap\n",
    "uni2_cfg = {\n",
    "    'model_name':'vit_giant_patch14_224','img_size':CFG.patch_size,'patch_size':14,'depth':24,\n",
    "    'num_heads':24,'init_values':1e-5,'embed_dim':CFG.morph_emb_dims,'mlp_ratio':2.66667*2,\n",
    "    'num_classes':0,'no_embed_class':True,'mlp_layer':timm.layers.SwiGLUPacked,\n",
    "    'act_layer':torch.nn.SiLU,'reg_tokens':8,'dynamic_img_size':True,\n",
    "}\n",
    "\n",
    "uni2 = timm.create_model(pretrained=False, **uni2_cfg)\n",
    "# (Optional) load UNI2 weights (same as training init)\n",
    "uni2_weights = os.path.join(CFG.model_dir, \"pytorch_model.bin\")\n",
    "if os.path.isfile(uni2_weights):\n",
    "    uni2.load_state_dict(torch.load(uni2_weights), strict=True)\n",
    "\n",
    "centre_idx = [119, 120, 135, 136] if CFG.level == 0 else [\n",
    "    102,103,104,105,118,119,120,121,134,135,136,137,150,151,152,153\n",
    "]\n",
    "vision_backbone = UNI2Wrapper(uni2, centre_idx)\n",
    "\n",
    "# Build model and load trained checkpoint\n",
    "model = CLIPGO(vision_backbone)\n",
    "ckpt_path = resolve_checkpoint()\n",
    "print(f\"→ Loading checkpoint: {ckpt_path}\")\n",
    "state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "missing, unexpected = model.load_state_dict(state[\"model\"], strict=True)\n",
    "if missing:   print(\"[load] missing keys:\", missing)\n",
    "if unexpected: print(\"[load] unexpected keys:\", unexpected)\n",
    "\n",
    "# Extract features\n",
    "cell_ids, v_raw, v_proj, t_raw, t_proj = extract_features(model, loader, device)\n",
    "\n",
    "# Quick sanity: alignment + cosine diag stats\n",
    "assert len(cell_ids) == len(dataset), \"Mismatch in number of extracted embeddings\"\n",
    "diag_cos = (v_proj @ t_proj.T).diagonal()\n",
    "print(f\"Diag cosine (proj): mean={diag_cos.mean():.3f}, std={diag_cos.std():.3f}\")\n",
    "\n",
    "# Save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8099ef0-0970-414e-955e-dc8c0a527b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved → /rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/models/fine_tuned/GoCLIP/features_lung.npz\n"
     ]
    }
   ],
   "source": [
    "save_npz(CFG.out_npz, cell_ids, v_raw, v_proj, t_raw, t_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9715007f-334c-4e06-b215-328e3a2fcc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "                             [--cancer {lung,breast,lymph_node,prostate,skin,ovarian,cervical}]\n",
      "                             [--ground_truth GROUND_TRUTH]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--num_workers NUM_WORKERS]\n",
      "                             [--precision {fp32,fp16,bf16}] [--root ROOT]\n",
      "                             [--go_dir GO_DIR] [--model_dir MODEL_DIR]\n",
      "                             [--ckpt_root CKPT_ROOT] --run_name RUN_NAME\n",
      "                             [--out_npz OUT_NPZ] [--target_mpp TARGET_MPP]\n",
      "                             [--seed SEED]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --run_name\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3468: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Feature extraction for CLIP-GO (UNI2 + CoCa)\n",
    "- Loads the same AnnData + GO sentences as training\n",
    "- Rebuilds the model, loads a checkpoint from a specific run\n",
    "- Extracts both raw and projected (L2-normalized) embeddings\n",
    "- Saves NPZ with: cell_id, vision_raw, vision_proj, text_raw, text_proj\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, glob, argparse\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import timm\n",
    "import open_clip\n",
    "import scanpy as sc\n",
    "import openslide\n",
    "\n",
    "# ──────────────────────────────\n",
    "# Args / CFG\n",
    "# ──────────────────────────────\n",
    "parser = argparse.ArgumentParser(description=\"Extract CLIP-GO features (UNI2 + CoCa)\")\n",
    "parser.add_argument(\"--cancer\", type=str, default=\"lung\",\n",
    "                    choices=[\"lung\",\"breast\",\"lymph_node\",\"prostate\",\"skin\",\"ovarian\",\"cervical\"])\n",
    "parser.add_argument(\"--ground_truth\", type=str, default=\"refined\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=256)\n",
    "parser.add_argument(\"--num_workers\", type=int, default=8)\n",
    "\n",
    "parser.add_argument(\"--precision\", type=str, choices=[\"fp32\",\"fp16\",\"bf16\"], default=\"bf16\",\n",
    "                    help=\"Extraction numerics: fp32, fp16 (AMP), or bf16 (autocast if supported).\")\n",
    "\n",
    "parser.add_argument(\"--root\", type=str, default=\"/rsrch9/home/plm/idso_fa1_pathology/TIER1/paul-xenium/public_data/10x_genomics\")\n",
    "parser.add_argument(\"--go_dir\", type=str, default=\"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/gene_ontology\")\n",
    "parser.add_argument(\"--model_dir\", type=str, default=\"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/models/public/UNI2-h\")\n",
    "parser.add_argument(\"--ckpt_root\", type=str, default=\"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/models/fine_tuned/GoCLIP\")\n",
    "parser.add_argument(\"--run_name\", type=str, required=True, help=\"Run folder name used during training\")\n",
    "parser.add_argument(\"--out_npz\", type=str, default=None, help=\"Output NPZ path (default auto)\")\n",
    "\n",
    "parser.add_argument(\"--target_mpp\", type=float, default=0.5)\n",
    "parser.add_argument(\"--seed\", type=int, default=1337)\n",
    "args = parser.parse_args()\n",
    "\n",
    "class CFG:\n",
    "    cancer = args.cancer\n",
    "    ground_truth = args.ground_truth\n",
    "    level = 0\n",
    "    batch_size = args.batch_size\n",
    "    num_workers = args.num_workers\n",
    "\n",
    "    # model dims\n",
    "    morph_emb_dims = 1536\n",
    "    projection_dim = 256\n",
    "    patch_size = 224\n",
    "\n",
    "    # text / CoCa\n",
    "    coca_model = \"coca_ViT-L-14\"\n",
    "    coca_pretrain = \"laion2B-s13b-b90k\"\n",
    "    context_len = 76\n",
    "    freeze_text = True  # no effect for no-grad, kept for parity\n",
    "\n",
    "    # paths\n",
    "    root = args.root\n",
    "    xenium_sample_dict = {\n",
    "        \"lung\":\"Xenium_Prime_Human_Lung_Cancer_FFPE_outs\",\n",
    "        \"breast\": \"Xenium_Prime_Breast_Cancer_FFPE_outs\",\n",
    "        \"lymph_node\": \"Xenium_Prime_Human_Lymph_Node_Reactive_FFPE_outs\",\n",
    "        \"prostate\": \"Xenium_Prime_Human_Prostate_FFPE_outs\",\n",
    "        \"skin\": \"Xenium_Prime_Human_Skin_FFPE_outs\",\n",
    "        \"ovarian\": \"Xenium_Prime_Ovarian_Cancer_FFPE_outs\",\n",
    "        \"cervical\": \"Xenium_Prime_Cervical_Cancer_FFPE_outs\",\n",
    "    }\n",
    "    go_dir = args.go_dir\n",
    "    model_dir = args.model_dir\n",
    "    ckpt_dir  = os.path.join(args.ckpt_root, args.cancer, args.run_name)  # point to a specific run\n",
    "    target_mpp = args.target_mpp\n",
    "\n",
    "    precision = args.precision\n",
    "\n",
    "    out_npz = args.out_npz or os.path.join(\n",
    "        args.ckpt_root, args.cancer, args.run_name, f\"features_{args.cancer}.npz\"\n",
    "    )\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(args.seed)\n",
    "\n",
    "# ──────────────────────────────\n",
    "# Layers (same as training)\n",
    "# ──────────────────────────────\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim: int, proj_dim: int = CFG.projection_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, proj_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(proj_dim, proj_dim)\n",
    "        self.ln  = nn.LayerNorm(proj_dim)\n",
    "    def forward(self, x):\n",
    "        h = self.fc1(x); x = self.act(h); x = self.fc2(x)\n",
    "        return self.ln(x + h)\n",
    "\n",
    "class UNI2Wrapper(nn.Module):\n",
    "    def __init__(self, uni2: nn.Module, centre_idx: List[int]):\n",
    "        super().__init__()\n",
    "        self.uni2 = uni2\n",
    "        self.centre_idx = torch.tensor(centre_idx, dtype=torch.long)\n",
    "        self.prefix_tokens = 9\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        tok = self.uni2.forward_features(x)               # (B, 265, 1536)\n",
    "        spatial = tok[:, self.prefix_tokens:, :]\n",
    "        centre  = spatial.index_select(1, self.centre_idx.to(x.device)).mean(1)\n",
    "        return centre                                     # (B,1536)\n",
    "\n",
    "class CellPatchTextDataset(Dataset):\n",
    "    def __init__(self, slide, cell_df: pd.DataFrame, sentences: pd.Series,\n",
    "                 transform, scale: float, patch_size: int = CFG.patch_size):\n",
    "        self.slide = slide\n",
    "        self.cells = cell_df.reset_index(drop=False)  # keep cell_id in \"index\"\n",
    "        self.sentences = sentences\n",
    "        self.tfm = transform\n",
    "        self.scale = scale\n",
    "        self.patch_size = patch_size\n",
    "    def __len__(self): return len(self.cells)\n",
    "    def _read_patch(self, x, y):\n",
    "        big = int(self.patch_size * self.scale)\n",
    "        tlx, tly = int(x - big/2), int(y - big/2)\n",
    "        patch = self.slide.read_region((tlx, tly), 0, (big, big)).convert(\"RGB\")\n",
    "        return patch.resize((self.patch_size, self.patch_size), Image.LANCZOS)\n",
    "    def __getitem__(self, idx):\n",
    "        row   = self.cells.iloc[idx]\n",
    "        patch = self._read_patch(row.x_centroid, row.y_centroid)\n",
    "        img_t = self.tfm(patch)\n",
    "        cell_id = row[\"index\"]\n",
    "        sent = self.sentences.loc[cell_id]\n",
    "        sent = \"\" if pd.isna(sent) else str(sent)\n",
    "        return {\"image\": img_t, \"text\": sent, \"cell_id\": cell_id}\n",
    "\n",
    "class CLIPGO(nn.Module):\n",
    "    def __init__(self, vision_backbone: nn.Module):\n",
    "        super().__init__()\n",
    "        self.context_len = CFG.context_len\n",
    "        self.freeze_text = CFG.freeze_text\n",
    "        # Vision\n",
    "        self.vision_encoder = vision_backbone\n",
    "        vision_dim = vision_backbone.uni2.embed_dim\n",
    "        self.vision_proj  = ProjectionHead(vision_dim, CFG.projection_dim)\n",
    "        # Text (CoCa)\n",
    "        self.text_encoder, _, _ = open_clip.create_model_and_transforms(\n",
    "            \"coca_ViT-L-14\", pretrained=\"laion2B-s13b-b90k\",\n",
    "            cache_dir=os.path.expanduser(\"~/.cache/open_clip\")\n",
    "        )\n",
    "        self.tokenizer = open_clip.get_tokenizer(\"coca_ViT-L-14\")\n",
    "        if self.freeze_text:\n",
    "            for p in self.text_encoder.parameters():\n",
    "                p.requires_grad_(False)\n",
    "        # Text width\n",
    "        with torch.no_grad():\n",
    "            dummy = self.tokenizer([\"dummy\"], context_length=self.context_len)\n",
    "            txt_feat = self.text_encoder.encode_text(dummy)\n",
    "        text_dim = txt_feat.shape[-1]\n",
    "        self.text_proj = ProjectionHead(text_dim, CFG.projection_dim)\n",
    "        # Temp (unused in extraction)\n",
    "        self.logit_scale = nn.Parameter(torch.log(torch.tensor(1/0.07)))\n",
    "\n",
    "# ──────────────────────────────\n",
    "# Helpers\n",
    "# ──────────────────────────────\n",
    "def get_slide_and_mpp(slide_dir: str):\n",
    "    tifs = sorted(glob.glob(os.path.join(slide_dir, \"**\", \"*he_image_registered*.ome.tif\"), recursive=True))\n",
    "    if not tifs:\n",
    "        tifs = sorted(glob.glob(os.path.join(slide_dir, \"**\", \"*.tif\"), recursive=True))\n",
    "        assert tifs, f\"No slide tif found under {slide_dir}\"\n",
    "    slide_path = tifs[0]\n",
    "    slide = openslide.open_slide(slide_path)\n",
    "    props = slide.properties\n",
    "    mpp = None\n",
    "    for key in (\"openslide.mpp-x\", \"aperio.MPP\", \"tiff.XResolution\"):\n",
    "        if key in props:\n",
    "            try:\n",
    "                mpp = float(props[key]); break\n",
    "            except Exception:\n",
    "                pass\n",
    "    if mpp is None:\n",
    "        mpp = CFG.target_mpp\n",
    "    return slide, mpp, slide_path\n",
    "\n",
    "def build_transforms():\n",
    "    return T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "    ])\n",
    "\n",
    "def resolve_checkpoint():\n",
    "    p = os.path.join(CFG.ckpt_dir, \"best.pth\")\n",
    "    if os.path.isfile(p):\n",
    "        return p\n",
    "    p = os.path.join(CFG.ckpt_dir, \"last.pth\")\n",
    "    if os.path.isfile(p):\n",
    "        return p\n",
    "    raise FileNotFoundError(f\"No checkpoint in {CFG.ckpt_dir} (looked for best.pth/last.pth)\")\n",
    "\n",
    "def save_npz(path: str, cell_ids, vis_raw, vis_proj, txt_raw, txt_proj):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    np.savez_compressed(\n",
    "        path,\n",
    "        cell_id=np.array(cell_ids),\n",
    "        vision_raw=vis_raw,\n",
    "        vision_proj=vis_proj,\n",
    "        text_raw=txt_raw,\n",
    "        text_proj=txt_proj,\n",
    "    )\n",
    "    print(f\"✓ Saved → {path}\")\n",
    "\n",
    "# ──────────────────────────────\n",
    "# Main\n",
    "# ──────────────────────────────\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data & sentences\n",
    "sample  = CFG.xenium_sample_dict[CFG.cancer]\n",
    "sample_dir = os.path.join(CFG.root, sample)\n",
    "adata_path = os.path.join(\n",
    "    sample_dir, \"preprocessed\", f\"fine_tune_{CFG.ground_truth}_v2\",\n",
    "    f\"processed_xenium_data_fine_tune_{CFG.ground_truth}_v2_annotated.h5ad\",\n",
    ")\n",
    "adata = sc.read_h5ad(adata_path)\n",
    "cell_df = adata.obs.copy()\n",
    "\n",
    "sent_path = f\"{CFG.go_dir}/{sample.replace('outs', 'GO.csv')}\"\n",
    "assert os.path.isfile(sent_path), f\"Missing sentences file: {sent_path}\"\n",
    "sentences = pd.read_csv(sent_path, index_col=\"cell_id\")[\"go_sentences\"].astype(str)\n",
    "sentences = sentences.reindex(cell_df.index).fillna(\"\")\n",
    "\n",
    "# Slide + scale\n",
    "slide, mpp, slide_path = get_slide_and_mpp(sample_dir)\n",
    "scale_factor = max(CFG.target_mpp / float(mpp), 1e-6)\n",
    "\n",
    "# Loader (no shuffle)\n",
    "transform = build_transforms()\n",
    "dataset = CellPatchTextDataset(slide, cell_df, sentences, transform,\n",
    "                               scale=scale_factor, patch_size=CFG.patch_size)\n",
    "loader  = DataLoader(dataset, batch_size=CFG.batch_size, shuffle=False,\n",
    "                     num_workers=CFG.num_workers, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "# Model (UNI2 + wrapper + CoCa)\n",
    "uni2_cfg = {\n",
    "    'model_name':'vit_giant_patch14_224','img_size':CFG.patch_size,'patch_size':14,'depth':24,\n",
    "    'num_heads':24,'init_values':1e-5,'embed_dim':CFG.morph_emb_dims,'mlp_ratio':2.66667*2,\n",
    "    'num_classes':0,'no_embed_class':True,'mlp_layer':timm.layers.SwiGLUPacked,\n",
    "    'act_layer':torch.nn.SiLU,'reg_tokens':8,'dynamic_img_size':True,\n",
    "}\n",
    "uni2 = timm.create_model(pretrained=False, **uni2_cfg)\n",
    "# (optional) load UNI2 init weights; checkpoint will overwrite anyway\n",
    "uni2_weights = os.path.join(CFG.model_dir, \"pytorch_model.bin\")\n",
    "if os.path.isfile(uni2_weights):\n",
    "    uni2.load_state_dict(torch.load(uni2_weights), strict=True)\n",
    "\n",
    "centre_idx = [119, 120, 135, 136] if CFG.level == 0 else [\n",
    "    102,103,104,105,118,119,120,121,134,135,136,137,150,151,152,153\n",
    "]\n",
    "vision_backbone = UNI2Wrapper(uni2, centre_idx)\n",
    "model = CLIPGO(vision_backbone)\n",
    "\n",
    "# Load checkpoint\n",
    "ckpt = resolve_checkpoint()\n",
    "print(f\"→ Loading checkpoint: {ckpt}\")\n",
    "state = torch.load(ckpt)\n",
    "missing, unexpected = model.load_state_dict(state[\"model\"], strict=True)\n",
    "if missing:   print(\"[load] missing keys:\", missing)\n",
    "if unexpected: print(\"[load] unexpected keys:\", unexpected)\n",
    "\n",
    "# Precision context for extraction\n",
    "use_cuda = (device.type == \"cuda\")\n",
    "req = CFG.precision.lower()\n",
    "if not use_cuda or req == \"fp32\":\n",
    "    # strict fp32: disable TF32 for apples-to-apples\n",
    "    if use_cuda:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    autocast_ctx = torch.no_grad()  # already no_grad globally, this is fine\n",
    "elif req == \"bf16\" and torch.cuda.is_bf16_supported():\n",
    "    autocast_ctx = torch.autocast(\"cuda\", dtype=torch.bfloat16)\n",
    "else:\n",
    "    # fp16 AMP\n",
    "    autocast_ctx = torch.cuda.amp.autocast(enabled=True)\n",
    "\n",
    "# Extract\n",
    "model.eval().to(device)\n",
    "ids = []\n",
    "vis_raw, vis_proj = [], []\n",
    "txt_raw, txt_proj = [], []\n",
    "\n",
    "pbar = tqdm(loader, desc=\"Extracting features\", dynamic_ncols=True)\n",
    "for batch in pbar:\n",
    "    imgs  = batch[\"image\"].to(device, non_blocking=True)\n",
    "    texts = batch[\"text\"]\n",
    "    batch_ids = batch[\"cell_id\"]\n",
    "    ids.extend(batch_ids)\n",
    "\n",
    "    with autocast_ctx:\n",
    "        # Vision\n",
    "        v_raw_t  = model.vision_encoder(imgs)                    # (B, 1536)\n",
    "        v_proj_t = F.normalize(model.vision_proj(v_raw_t), dim=-1)\n",
    "        # Text\n",
    "        tokens = model.tokenizer(texts, context_length=model.context_len).to(device)\n",
    "        t_raw_t  = model.text_encoder.encode_text(tokens)        # (B, text_dim)\n",
    "        t_proj_t = F.normalize(model.text_proj(t_raw_t), dim=-1)\n",
    "\n",
    "    vis_raw.append(v_raw_t.float().cpu())\n",
    "    vis_proj.append(v_proj_t.float().cpu())\n",
    "    txt_raw.append(t_raw_t.float().cpu())\n",
    "    txt_proj.append(t_proj_t.float().cpu())\n",
    "\n",
    "vis_raw  = torch.cat(vis_raw, 0).numpy()\n",
    "vis_proj = torch.cat(vis_proj, 0).numpy()\n",
    "txt_raw  = torch.cat(txt_raw, 0).numpy()\n",
    "txt_proj = torch.cat(txt_proj, 0).numpy()\n",
    "\n",
    "# Save\n",
    "save_npz(CFG.out_npz, ids, vis_raw, vis_proj, txt_raw, txt_proj)\n",
    "\n",
    "# Quick sanity\n",
    "diag_cos = (vis_proj @ txt_proj.T).diagonal()\n",
    "print(f\"Diag cosine (proj): mean={diag_cos.mean():.3f}, std={diag_cos.std():.3f}, n={diag_cos.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ffc3c-5bd5-4207-acd0-5d126de04026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "N cells: 244659\n",
      "→ Loading checkpoint: /rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/models/fine_tuned/GoCLIP/lung/lung_bf16_freeze_e50/best.pth\n",
      "Loaded epoch:  47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|██████████| 956/956 [05:23<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved → /rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/embeddings/public_data/Xenium_Prime_Human_Lung_Cancer_FFPE_outs/GoCLIP/lung_bf16_freeze_e50/features.npz\n"
     ]
    }
   ],
   "source": [
    "# ===== Notebook Evaluation: CLIP-GO (UNI2 + CoCa) =====\n",
    "import os, glob\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from contextlib import nullcontext\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import timm\n",
    "import open_clip\n",
    "import torchvision.transforms as T\n",
    "import scanpy as sc\n",
    "import openslide\n",
    "\n",
    "\n",
    "# ──────────────────────────────\n",
    "# Config (class style, notebook-friendly)\n",
    "# ──────────────────────────────\n",
    "class CFG:\n",
    "    # choose the trained run to evaluate\n",
    "    cancer      = \"lung\"\n",
    "    run_name    = \"lung_bf16_freeze_e50\"\n",
    "    ground_truth= \"refined\"\n",
    "\n",
    "    # IO\n",
    "    root        = \"/rsrch9/home/plm/idso_fa1_pathology/TIER1/paul-xenium/public_data/10x_genomics\"\n",
    "    go_dir      = \"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/gene_ontology\"\n",
    "    ckpt_root   = \"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/models/fine_tuned/GoCLIP\"\n",
    "    \n",
    "\n",
    "    # model dims\n",
    "    level            = 0                   # 0 → 4-center tokens\n",
    "    morph_emb_dims   = 1536\n",
    "    projection_dim   = 256\n",
    "    patch_size       = 224\n",
    "    context_len      = 76\n",
    "\n",
    "    # loader (notebook-friendly)\n",
    "    batch_size       = 256    # lower if OOM\n",
    "    num_workers      = 16      # notebooks: keep 0\n",
    "    target_mpp       = 0.5\n",
    "\n",
    "    # eval precision: \"fp32\" | \"fp16\" | \"bf16\"\n",
    "    precision        = \"bf16\"\n",
    "\n",
    "    # text / CoCa (same as training)\n",
    "    coca_model       = \"coca_ViT-L-14\"\n",
    "    coca_pretrain    = \"laion2B-s13b-b90k\"\n",
    "    freeze_text      = True   # no-grad anyway; kept for parity\n",
    "\n",
    "    # mapping\n",
    "    xenium_sample_dict = {\n",
    "        \"lung\":\"Xenium_Prime_Human_Lung_Cancer_FFPE_outs\",\n",
    "        \"breast\": \"Xenium_Prime_Breast_Cancer_FFPE_outs\",\n",
    "        \"lymph_node\": \"Xenium_Prime_Human_Lymph_Node_Reactive_FFPE_outs\",\n",
    "        \"prostate\": \"Xenium_Prime_Human_Prostate_FFPE_outs\",\n",
    "        \"skin\": \"Xenium_Prime_Human_Skin_FFPE_outs\",\n",
    "        \"ovarian\": \"Xenium_Prime_Ovarian_Cancer_FFPE_outs\",\n",
    "        \"cervical\": \"Xenium_Prime_Cervical_Cancer_FFPE_outs\",\n",
    "    }\n",
    "    \n",
    "    out_npz     = f\"{os.path.dirname(go_dir)}/embeddings/public_data/{xenium_sample_dict[cancer]}/GoCLIP/{run_name}/features.npz\" \n",
    "\n",
    "# Repro\n",
    "def set_seed(seed=1337):\n",
    "    import random\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "set_seed(1337)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n",
    "# ──────────────────────────────\n",
    "# Layers (match training)\n",
    "# ──────────────────────────────\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim: int, proj_dim: int = CFG.projection_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, proj_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(proj_dim, proj_dim)\n",
    "        self.ln  = nn.LayerNorm(proj_dim)\n",
    "    def forward(self, x):\n",
    "        h = self.fc1(x); x = self.act(h); x = self.fc2(x)\n",
    "        return self.ln(x + h)\n",
    "\n",
    "class UNI2Wrapper(nn.Module):\n",
    "    def __init__(self, uni2: nn.Module, centre_idx: List[int]):\n",
    "        super().__init__()\n",
    "        self.uni2 = uni2\n",
    "        self.centre_idx = torch.tensor(centre_idx, dtype=torch.long)\n",
    "        self.prefix_tokens = 9\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        tok = self.uni2.forward_features(x)               # (B, 265, 1536)\n",
    "        spatial = tok[:, self.prefix_tokens:, :]\n",
    "        centre  = spatial.index_select(1, self.centre_idx.to(x.device)).mean(1)\n",
    "        return centre                                     # (B,1536)\n",
    "\n",
    "class CLIPGO(nn.Module):\n",
    "    def __init__(self, vision_backbone: nn.Module):\n",
    "        super().__init__()\n",
    "        self.context_len = CFG.context_len\n",
    "        self.freeze_text = CFG.freeze_text\n",
    "\n",
    "        # Vision\n",
    "        self.vision_encoder = vision_backbone\n",
    "        vision_dim = vision_backbone.uni2.embed_dim\n",
    "        self.vision_proj  = ProjectionHead(vision_dim, CFG.projection_dim)\n",
    "\n",
    "        # Text (CoCa)\n",
    "        self.text_encoder, _, _ = open_clip.create_model_and_transforms(\n",
    "            CFG.coca_model, pretrained=CFG.coca_pretrain,\n",
    "            cache_dir=os.path.expanduser(\"~/.cache/open_clip\")\n",
    "        )\n",
    "        self.tokenizer = open_clip.get_tokenizer(CFG.coca_model)\n",
    "        if self.freeze_text:\n",
    "            for p in self.text_encoder.parameters():\n",
    "                p.requires_grad_(False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = self.tokenizer([\"dummy\"], context_length=self.context_len)\n",
    "            txt_feat = self.text_encoder.encode_text(dummy)\n",
    "        text_dim = txt_feat.shape[-1]\n",
    "        self.text_proj = ProjectionHead(text_dim, CFG.projection_dim)\n",
    "\n",
    "        self.logit_scale = nn.Parameter(torch.log(torch.tensor(1/0.07)))  # not used in eval\n",
    "\n",
    "\n",
    "# ──────────────────────────────\n",
    "# Data helpers\n",
    "# ──────────────────────────────\n",
    "def get_slide_and_mpp(slide_dir: str):\n",
    "    tifs = sorted(glob.glob(os.path.join(slide_dir, \"**\", \"*he_image_registered*.ome.tif\"), recursive=True))\n",
    "    if not tifs:\n",
    "        tifs = sorted(glob.glob(os.path.join(slide_dir, \"**\", \"*.tif\"), recursive=True))\n",
    "        assert tifs, f\"No slide tif found under {slide_dir}\"\n",
    "    slide_path = tifs[0]\n",
    "    slide = openslide.open_slide(slide_path)\n",
    "    props = slide.properties\n",
    "    mpp = None\n",
    "    for key in (\"openslide.mpp-x\", \"aperio.MPP\", \"tiff.XResolution\"):\n",
    "        if key in props:\n",
    "            try:\n",
    "                mpp = float(props[key]); break\n",
    "            except Exception:\n",
    "                pass\n",
    "    if mpp is None:\n",
    "        mpp = CFG.target_mpp\n",
    "    return slide, mpp, slide_path\n",
    "\n",
    "def build_transforms():\n",
    "    return T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "    ])\n",
    "\n",
    "class CellPatchTextDataset(Dataset):\n",
    "    def __init__(self, slide, cell_df: pd.DataFrame, sentences: pd.Series,\n",
    "                 transform, scale: float, patch_size: int = CFG.patch_size):\n",
    "        self.slide = slide\n",
    "        self.cells = cell_df.reset_index(drop=False)  # keep cell_id in \"index\"\n",
    "        self.sentences = sentences\n",
    "        self.tfm = transform\n",
    "        self.scale = scale\n",
    "        self.patch_size = patch_size\n",
    "    def __len__(self): return len(self.cells)\n",
    "    def _read_patch(self, x, y):\n",
    "        big = int(self.patch_size * self.scale)\n",
    "        tlx, tly = int(x - big/2), int(y - big/2)\n",
    "        patch = self.slide.read_region((tlx, tly), 0, (big, big)).convert(\"RGB\")\n",
    "        return patch.resize((self.patch_size, self.patch_size), Image.LANCZOS)\n",
    "    def __getitem__(self, idx):\n",
    "        row   = self.cells.iloc[idx]\n",
    "        patch = self._read_patch(row.x_centroid, row.y_centroid)\n",
    "        img_t = self.tfm(patch)\n",
    "        cell_id = row[\"index\"]\n",
    "        sent = self.sentences.loc[cell_id]\n",
    "        sent = \"\" if pd.isna(sent) else str(sent)\n",
    "        return {\"image\": img_t, \"text\": sent, \"cell_id\": cell_id}\n",
    "\n",
    "def resolve_checkpoint():\n",
    "    ckpt_dir = os.path.join(CFG.ckpt_root, CFG.cancer, CFG.run_name)\n",
    "    for name in [\"best.pth\", \"last.pth\"]:\n",
    "        p = os.path.join(ckpt_dir, name)\n",
    "        if os.path.isfile(p): return p\n",
    "    raise FileNotFoundError(f\"No checkpoint in {ckpt_dir} (best.pth/last.pth not found)\")\n",
    "\n",
    "def save_npz(path: str, cell_ids, vis_raw, vis_proj, txt_raw, txt_proj):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    np.savez_compressed(\n",
    "        path,\n",
    "        cell_id=np.array(cell_ids),\n",
    "        vision_raw=vis_raw,\n",
    "        vision_proj=vis_proj,\n",
    "        text_raw=txt_raw,\n",
    "        text_proj=txt_proj,\n",
    "    )\n",
    "    print(f\"✓ Saved → {path}\")\n",
    "\n",
    "\n",
    "# ──────────────────────────────\n",
    "# Build data & loader (no shuffle)\n",
    "# ──────────────────────────────\n",
    "sample  = CFG.xenium_sample_dict[CFG.cancer]\n",
    "sample_dir = os.path.join(CFG.root, sample)\n",
    "\n",
    "adata_path = os.path.join(\n",
    "    sample_dir, \"preprocessed\", f\"fine_tune_{CFG.ground_truth}_v2\",\n",
    "    f\"processed_xenium_data_fine_tune_{CFG.ground_truth}_v2_annotated.h5ad\",\n",
    ")\n",
    "adata   = sc.read_h5ad(adata_path)\n",
    "cell_df = adata.obs.copy()\n",
    "\n",
    "sent_path = f\"{CFG.go_dir}/{sample.replace('outs', 'GO.csv')}\"\n",
    "assert os.path.isfile(sent_path), f\"Missing sentences file: {sent_path}\"\n",
    "sentences = pd.read_csv(sent_path, index_col=\"cell_id\")[\"go_sentences\"].astype(str)\n",
    "sentences = sentences.reindex(cell_df.index).fillna(\"\")\n",
    "\n",
    "slide, mpp, _ = get_slide_and_mpp(sample_dir)\n",
    "scale_factor = max(CFG.target_mpp / float(mpp), 1e-6)\n",
    "\n",
    "transform = build_transforms()\n",
    "dataset = CellPatchTextDataset(slide, cell_df, sentences, transform, scale=scale_factor)\n",
    "loader  = DataLoader(dataset,\n",
    "                     batch_size=CFG.batch_size,\n",
    "                     shuffle=False,\n",
    "                     num_workers=CFG.num_workers,\n",
    "                     pin_memory=True,\n",
    "                     persistent_workers=False)\n",
    "\n",
    "print(\"N cells:\", len(dataset))\n",
    "\n",
    "\n",
    "# ──────────────────────────────\n",
    "# Build model, load checkpoint\n",
    "# ──────────────────────────────\n",
    "uni2_cfg = {\n",
    "    'model_name':'vit_giant_patch14_224','img_size':CFG.patch_size,'patch_size':14,'depth':24,\n",
    "    'num_heads':24,'init_values':1e-5,'embed_dim':CFG.morph_emb_dims,'mlp_ratio':2.66667*2,\n",
    "    'num_classes':0,'no_embed_class':True,'mlp_layer':timm.layers.SwiGLUPacked,\n",
    "    'act_layer':torch.nn.SiLU,'reg_tokens':8,'dynamic_img_size':True,\n",
    "}\n",
    "uni2 = timm.create_model(pretrained=False, **uni2_cfg)\n",
    "centre_idx = [119, 120, 135, 136] if CFG.level == 0 else [\n",
    "    102,103,104,105,118,119,120,121,134,135,136,137,150,151,152,153\n",
    "]\n",
    "vision_backbone = UNI2Wrapper(uni2, centre_idx)\n",
    "model = CLIPGO(vision_backbone)\n",
    "\n",
    "ckpt_path = resolve_checkpoint()\n",
    "print(\"→ Loading checkpoint:\", ckpt_path)\n",
    "state = torch.load(ckpt_path)\n",
    "print(\"Loaded epoch: \", state[\"epoch\"])\n",
    "\n",
    "missing, unexpected = model.load_state_dict(state[\"model\"], strict=True)\n",
    "if missing:   print(\"[load] missing keys:\", missing)\n",
    "if unexpected: print(\"[load] unexpected keys:\", unexpected)\n",
    "\n",
    "model = model.to(device).eval()\n",
    "\n",
    "\n",
    "# ──────────────────────────────\n",
    "# Precision context (eval only)\n",
    "# ──────────────────────────────\n",
    "req = CFG.precision.lower()\n",
    "use_cuda = (device.type == \"cuda\")\n",
    "if not use_cuda or req == \"fp32\":\n",
    "    if use_cuda:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    autocast_ctx = nullcontext()\n",
    "elif req == \"bf16\" and torch.cuda.is_bf16_supported():\n",
    "    autocast_ctx = torch.autocast(\"cuda\", dtype=torch.bfloat16)\n",
    "else:\n",
    "    autocast_ctx = torch.cuda.amp.autocast(True)  # fp16\n",
    "\n",
    "\n",
    "# ──────────────────────────────\n",
    "# Extract features\n",
    "# ──────────────────────────────\n",
    "ids = []\n",
    "vis_raw, vis_proj = [], []\n",
    "txt_raw, txt_proj = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    pbar = tqdm(loader, desc=\"Extracting\", dynamic_ncols=True)\n",
    "    for batch in pbar:\n",
    "        imgs  = batch[\"image\"].to(device, non_blocking=True)\n",
    "        texts = batch[\"text\"]\n",
    "        ids.extend(batch[\"cell_id\"])\n",
    "\n",
    "        with autocast_ctx:\n",
    "            v_raw_t  = model.vision_encoder(imgs)                    # (B, 1536)\n",
    "            v_proj_t = F.normalize(model.vision_proj(v_raw_t), dim=-1)\n",
    "            tokens   = model.tokenizer(texts, context_length=model.context_len).to(device)\n",
    "            t_raw_t  = model.text_encoder.encode_text(tokens)        # (B, text_dim)\n",
    "            t_proj_t = F.normalize(model.text_proj(t_raw_t), dim=-1)\n",
    "\n",
    "        vis_raw.append(v_raw_t.float().cpu())\n",
    "        vis_proj.append(v_proj_t.float().cpu())\n",
    "        txt_raw.append(t_raw_t.float().cpu())\n",
    "        txt_proj.append(t_proj_t.float().cpu())\n",
    "\n",
    "vis_raw  = torch.cat(vis_raw, 0).numpy()\n",
    "vis_proj = torch.cat(vis_proj, 0).numpy()\n",
    "txt_raw  = torch.cat(txt_raw, 0).numpy()\n",
    "txt_proj = torch.cat(txt_proj, 0).numpy()\n",
    "\n",
    "# save\n",
    "out_npz = CFG.out_npz\n",
    "os.makedirs(os.path.dirname(out_npz), exist_ok=True)\n",
    "np.savez_compressed(out_npz,\n",
    "    cell_id=np.array(ids),\n",
    "    vision_raw=vis_raw,\n",
    "    vision_proj=vis_proj,\n",
    "    text_raw=txt_raw,\n",
    "    text_proj=txt_proj,\n",
    ")\n",
    "print(\"✓ Saved →\", out_npz)\n",
    "\n",
    "# sanity stat\n",
    "diag_cos = (vis_proj @ txt_proj.T).diagonal()\n",
    "print(f\"Diag cosine (proj): mean={diag_cos.mean():.3f}, std={diag_cos.std():.3f}, n={diag_cos.size}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190e7222-7f02-496a-8e65-3af015dcdf18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phacosta (py3.10.12)",
   "language": "python",
   "name": "phacosta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
