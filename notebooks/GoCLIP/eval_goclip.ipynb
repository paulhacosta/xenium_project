{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80ad0d08-4a91-4574-900a-4d6ee2105557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Feature extraction for CLIP-GO (UNI2 + CoCa)\n",
    "===========================================\n",
    "\n",
    "• Reuses the training-time wiring (CFG, UNI2Wrapper, CLIPGO).\n",
    "• Loads the same AnnData + sentences CSV.\n",
    "• Builds a non-shuffled DataLoader.\n",
    "• Loads best/last checkpoint.\n",
    "• Saves NPZ with:\n",
    "    - cell_id\n",
    "    - vision_raw  : UNI2 pooled (4 center tokens), dim=1536\n",
    "    - vision_proj : projected + L2-normalized, dim=256\n",
    "    - text_raw    : CoCa encode_text output, dim≈768/1024 (depends on model)\n",
    "    - text_proj   : projected + L2-normalized, dim=256\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os, glob, math\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import timm\n",
    "import open_clip\n",
    "import scanpy as sc\n",
    "import openslide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77c8a8c1-ac3d-4e92-b07e-6ca225b6c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# CFG (match your training style)\n",
    "# -----------------------------------------------------------------------------\n",
    "class CFG:\n",
    "    # data\n",
    "    cancer = \"lung\"\n",
    "    ground_truth = \"refined\"\n",
    "    level = 0                 # UNI2 spatial token level (0 → 4-center tokens)\n",
    "    batch_size = 256\n",
    "    num_workers = 8\n",
    "\n",
    "    # embeddings / model dims\n",
    "    morph_emb_dims = 1536\n",
    "    projection_dim = 256\n",
    "    patch_size = 224\n",
    "\n",
    "    # Text / CoCa\n",
    "    coca_model = \"coca_ViT-L-14\"\n",
    "    coca_pretrain = \"laion2B-s13b-b90k\"\n",
    "    context_len = 76\n",
    "    freeze_text = True  # only matters if you change the model; extraction is no-grad\n",
    "\n",
    "    # paths (mirror your training)\n",
    "    root = \"/rsrch9/home/plm/idso_fa1_pathology/TIER1/paul-xenium/public_data/10x_genomics\"\n",
    "    xenium_sample_dict = {\n",
    "        \"lung\":\"Xenium_Prime_Human_Lung_Cancer_FFPE_outs\",\n",
    "        \"breast\": \"Xenium_Prime_Breast_Cancer_FFPE_outs\",\n",
    "        \"lymph_node\": \"Xenium_Prime_Human_Lymph_Node_Reactive_FFPE_outs\",\n",
    "        \"prostate\": \"Xenium_Prime_Human_Prostate_FFPE_outs\",\n",
    "        \"skin\": \"Xenium_Prime_Human_Skin_FFPE_outs\",\n",
    "        \"ovarian\": \"Xenium_Prime_Ovarian_Cancer_FFPE_outs\",\n",
    "        \"cervical\": \"Xenium_Prime_Cervical_Cancer_FFPE_outs\",\n",
    "    }\n",
    "    go_dir = \"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/gene_ontology\"\n",
    "    model_dir = \"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/models/public/UNI2-h\"\n",
    "    ckpt_dir  = \"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/models/fine_tuned/GoCLIP\"\n",
    "\n",
    "    target_mpp = 0.5  # target µm/px (≈20×)\n",
    "\n",
    "    # output\n",
    "    out_npz = \"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/models/fine_tuned/GoCLIP/features_lung.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92f1f73f-0d85-4b0b-909e-c304d847314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Projection head (same as training)\n",
    "# -----------------------------------------------------------------------------\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim: int, proj_dim: int = CFG.projection_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, proj_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(proj_dim, proj_dim)\n",
    "        self.ln  = nn.LayerNorm(proj_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.fc1(x)\n",
    "        x = self.act(h)\n",
    "        x = self.fc2(x)\n",
    "        return self.ln(x + h)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# UNI2 wrapper (4-centre token pooling; same as training)\n",
    "# -----------------------------------------------------------------------------\n",
    "class UNI2Wrapper(nn.Module):\n",
    "    def __init__(self, uni2: nn.Module, centre_idx: List[int]):\n",
    "        super().__init__()\n",
    "        self.uni2 = uni2\n",
    "        self.centre_idx = torch.tensor(centre_idx, dtype=torch.long)\n",
    "        self.prefix_tokens = 9\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        tok = self.uni2.forward_features(x)               # (B, 265, 1536)\n",
    "        spatial = tok[:, self.prefix_tokens :, :]\n",
    "        centre  = spatial.index_select(1, self.centre_idx.to(x.device)).mean(1)\n",
    "        return centre                                     # (B,1536)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Dataset: cell-centred patch + GO sentence (returns cell_id too)\n",
    "# -----------------------------------------------------------------------------\n",
    "class CellPatchTextDataset(Dataset):\n",
    "    def __init__(self, slide, cell_df: pd.DataFrame, sentences: pd.Series,\n",
    "                 transform, scale: float, patch_size: int = CFG.patch_size):\n",
    "        self.slide = slide\n",
    "        self.cells = cell_df.reset_index(drop=False)  # keep cell_id in column \"index\"\n",
    "        self.sentences = sentences\n",
    "        self.tfm = transform\n",
    "        self.scale = scale\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cells)\n",
    "\n",
    "    def _read_patch(self, x, y):\n",
    "        big = int(self.patch_size * self.scale)\n",
    "        tlx, tly = int(x - big/2), int(y - big/2)\n",
    "        patch = self.slide.read_region((tlx, tly), 0, (big, big)).convert(\"RGB\")\n",
    "        return patch.resize((self.patch_size, self.patch_size), Image.LANCZOS)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row   = self.cells.iloc[idx]\n",
    "        patch = self._read_patch(row.x_centroid, row.y_centroid)\n",
    "        img_t = self.tfm(patch)\n",
    "        cell_id = row[\"index\"]\n",
    "        sent = self.sentences.loc[cell_id]\n",
    "        sent = \"\" if pd.isna(sent) else str(sent)\n",
    "        return {\"image\": img_t, \"text\": sent, \"cell_id\": cell_id}\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CLIP-GO (UNI2 + CoCa) – same as training\n",
    "# -----------------------------------------------------------------------------\n",
    "class CLIPGO(nn.Module):\n",
    "    def __init__(self, vision_backbone: nn.Module,\n",
    "                 coca_model: str = CFG.coca_model,\n",
    "                 coca_pretrain: str = CFG.coca_pretrain,\n",
    "                 proj_dim: int = CFG.projection_dim,\n",
    "                 context_len: int = CFG.context_len,\n",
    "                 freeze_text: bool = CFG.freeze_text):\n",
    "        super().__init__()\n",
    "        self.context_len = context_len\n",
    "        self.freeze_text = freeze_text\n",
    "\n",
    "        # Vision branch\n",
    "        self.vision_encoder = vision_backbone\n",
    "        vision_dim = vision_backbone.uni2.embed_dim  # 1536 for ViT-Giant\n",
    "        self.vision_proj  = ProjectionHead(vision_dim, proj_dim)\n",
    "\n",
    "        # Text branch (CoCa causal)\n",
    "        self.text_encoder, _, _ = open_clip.create_model_and_transforms(\n",
    "            coca_model, pretrained=coca_pretrain,\n",
    "            cache_dir=os.path.expanduser(\"~/.cache/open_clip\")\n",
    "        )\n",
    "        self.tokenizer = open_clip.get_tokenizer(coca_model)\n",
    "        if freeze_text:\n",
    "            for p in self.text_encoder.parameters():\n",
    "                p.requires_grad_(False)\n",
    "\n",
    "        # Determine text width dynamically\n",
    "        with torch.no_grad():\n",
    "            dummy = self.tokenizer([\"dummy\"], context_length=context_len)\n",
    "            txt_feat = self.text_encoder.encode_text(dummy)\n",
    "        text_dim = txt_feat.shape[-1]\n",
    "        self.text_proj = ProjectionHead(text_dim, proj_dim)\n",
    "\n",
    "        # CLIP temperature (not used directly for extraction but kept for completeness)\n",
    "        self.logit_scale = nn.Parameter(torch.log(torch.tensor(1/0.07)))\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helpers: slide/MPP + transforms + checkpoint\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_slide_and_mpp(slide_dir: str):\n",
    "    tifs = sorted(glob.glob(os.path.join(slide_dir, \"**\", \"*he_image_registered*.ome.tif\"), recursive=True))\n",
    "    if not tifs:\n",
    "        tifs = sorted(glob.glob(os.path.join(slide_dir, \"**\", \"*.tif\"), recursive=True))\n",
    "        assert tifs, f\"No slide tif found under {slide_dir}\"\n",
    "    slide_path = tifs[0]\n",
    "    slide = openslide.open_slide(slide_path)\n",
    "\n",
    "    # mpp search (robust)\n",
    "    props = slide.properties\n",
    "    mpp = None\n",
    "    for key in (\"openslide.mpp-x\", \"aperio.MPP\", \"tiff.XResolution\"):\n",
    "        if key in props:\n",
    "            try:\n",
    "                mpp = float(props[key]); break\n",
    "            except Exception:\n",
    "                pass\n",
    "    if mpp is None:\n",
    "        mpp = CFG.target_mpp\n",
    "    return slide, mpp, slide_path\n",
    "\n",
    "\n",
    "def build_transforms():\n",
    "    return T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "    ])\n",
    "\n",
    "\n",
    "def resolve_checkpoint():\n",
    "    cands = [\n",
    "        os.path.join(CFG.ckpt_dir, \"best.pth\"),\n",
    "        os.path.join(CFG.ckpt_dir, CFG.cancer, \"best.pth\"),\n",
    "        os.path.join(CFG.ckpt_dir, \"last.pth\"),\n",
    "    ]\n",
    "    for p in cands:\n",
    "        if os.path.isfile(p):\n",
    "            return p\n",
    "    raise FileNotFoundError(f\"No checkpoint found in {CFG.ckpt_dir} (tried best.pth / last.pth).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6df11cdb-05b0-4a61-88c9-a68aba50329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Feature extraction\n",
    "# -----------------------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def extract_features(model: CLIPGO, loader: DataLoader, device: torch.device):\n",
    "    model.eval().to(device)\n",
    "    ids = []\n",
    "    vis_raw, vis_proj = [], []\n",
    "    txt_raw, txt_proj = [], []\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"Extracting features\", dynamic_ncols=True)\n",
    "    for batch in pbar:\n",
    "        imgs  = batch[\"image\"].to(device, non_blocking=True)\n",
    "        texts = batch[\"text\"]\n",
    "        batch_ids = batch[\"cell_id\"]\n",
    "        ids.extend(batch_ids)\n",
    "\n",
    "        # Vision\n",
    "        v_raw  = model.vision_encoder(imgs)                    # (B, 1536)\n",
    "        v_proj = F.normalize(model.vision_proj(v_raw), dim=-1) # (B, 256)\n",
    "\n",
    "        # Text\n",
    "        tokens = model.tokenizer(texts, context_length=model.context_len).to(device)\n",
    "        t_raw  = model.text_encoder.encode_text(tokens)        # (B, text_dim)\n",
    "        t_proj = F.normalize(model.text_proj(t_raw), dim=-1)   # (B, 256)\n",
    "\n",
    "        vis_raw.append(v_raw.cpu());   vis_proj.append(v_proj.cpu())\n",
    "        txt_raw.append(t_raw.cpu());   txt_proj.append(t_proj.cpu())\n",
    "\n",
    "    vis_raw  = torch.cat(vis_raw, 0).numpy()\n",
    "    vis_proj = torch.cat(vis_proj, 0).numpy()\n",
    "    txt_raw  = torch.cat(txt_raw, 0).numpy()\n",
    "    txt_proj = torch.cat(txt_proj, 0).numpy()\n",
    "    return ids, vis_raw, vis_proj, txt_raw, txt_proj\n",
    "\n",
    "\n",
    "def save_npz(path: str, cell_ids, vis_raw, vis_proj, txt_raw, txt_proj):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    np.savez_compressed(\n",
    "        path,\n",
    "        cell_id=np.array(cell_ids),\n",
    "        vision_raw=vis_raw,\n",
    "        vision_proj=vis_proj,\n",
    "        text_raw=txt_raw,\n",
    "        text_proj=txt_proj,\n",
    "    )\n",
    "    print(f\"✓ Saved → {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c9fa9f3-86e3-4bc9-9cac-2b0e4ee28cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Loading checkpoint: /rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/models/fine_tuned/GoCLIP/best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 956/956 [1:28:51<00:00,  5.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diag cosine (proj): mean=0.486, std=0.054\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Main\n",
    "# -----------------------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Resolve dataset paths\n",
    "sample  = CFG.xenium_sample_dict[CFG.cancer]\n",
    "sample_dir = os.path.join(CFG.root, sample)\n",
    "\n",
    "# Load AnnData (expects x_centroid / y_centroid in .obs)\n",
    "adata_path = os.path.join(\n",
    "    sample_dir,\n",
    "    \"preprocessed\",\n",
    "    f\"fine_tune_{CFG.ground_truth}_v2\",\n",
    "    f\"processed_xenium_data_fine_tune_{CFG.ground_truth}_v2_annotated.h5ad\",\n",
    ")\n",
    "adata = sc.read_h5ad(adata_path)\n",
    "cell_df = adata.obs.copy()  # index = cell_id\n",
    "\n",
    "# Sentences: same CSV logic as your training\n",
    "sent_path = f\"{CFG.go_dir}/{sample.replace('outs', 'GO.csv')}\"\n",
    "assert os.path.isfile(sent_path), f\"Missing sentences file: {sent_path}\"\n",
    "sentences = pd.read_csv(sent_path, index_col=\"cell_id\")[\"go_sentences\"].astype(str)\n",
    "sentences = sentences.reindex(cell_df.index).fillna(\"\")\n",
    "\n",
    "# Slide + scale\n",
    "slide, mpp, slide_path = get_slide_and_mpp(sample_dir)\n",
    "scale_factor = max(CFG.target_mpp / float(mpp), 1e-6)\n",
    "\n",
    "# Transforms and DataLoader (no shuffle, keep all)\n",
    "transform = build_transforms()\n",
    "dataset = CellPatchTextDataset(slide, cell_df, sentences, transform,\n",
    "                               scale=scale_factor, patch_size=CFG.patch_size)\n",
    "loader  = DataLoader(dataset, batch_size=CFG.batch_size, shuffle=False,\n",
    "                     num_workers=CFG.num_workers, pin_memory=True,\n",
    "                     persistent_workers=True)\n",
    "\n",
    "# Build vision backbone (UNI2) and wrap\n",
    "uni2_cfg = {\n",
    "    'model_name':'vit_giant_patch14_224','img_size':CFG.patch_size,'patch_size':14,'depth':24,\n",
    "    'num_heads':24,'init_values':1e-5,'embed_dim':CFG.morph_emb_dims,'mlp_ratio':2.66667*2,\n",
    "    'num_classes':0,'no_embed_class':True,'mlp_layer':timm.layers.SwiGLUPacked,\n",
    "    'act_layer':torch.nn.SiLU,'reg_tokens':8,'dynamic_img_size':True,\n",
    "}\n",
    "\n",
    "uni2 = timm.create_model(pretrained=False, **uni2_cfg)\n",
    "# (Optional) load UNI2 weights (same as training init)\n",
    "uni2_weights = os.path.join(CFG.model_dir, \"pytorch_model.bin\")\n",
    "if os.path.isfile(uni2_weights):\n",
    "    uni2.load_state_dict(torch.load(uni2_weights, map_location=\"cpu\"), strict=False)\n",
    "\n",
    "centre_idx = [119, 120, 135, 136] if CFG.level == 0 else [\n",
    "    102,103,104,105,118,119,120,121,134,135,136,137,150,151,152,153\n",
    "]\n",
    "vision_backbone = UNI2Wrapper(uni2, centre_idx)\n",
    "\n",
    "# Build model and load trained checkpoint\n",
    "model = CLIPGO(vision_backbone)\n",
    "ckpt_path = resolve_checkpoint()\n",
    "print(f\"→ Loading checkpoint: {ckpt_path}\")\n",
    "state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "missing, unexpected = model.load_state_dict(state[\"model\"], strict=False)\n",
    "if missing:   print(\"[load] missing keys:\", missing)\n",
    "if unexpected: print(\"[load] unexpected keys:\", unexpected)\n",
    "\n",
    "# Extract features\n",
    "cell_ids, v_raw, v_proj, t_raw, t_proj = extract_features(model, loader, device)\n",
    "\n",
    "# Quick sanity: alignment + cosine diag stats\n",
    "assert len(cell_ids) == len(dataset), \"Mismatch in number of extracted embeddings\"\n",
    "diag_cos = (v_proj @ t_proj.T).diagonal()\n",
    "print(f\"Diag cosine (proj): mean={diag_cos.mean():.3f}, std={diag_cos.std():.3f}\")\n",
    "\n",
    "# Save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8099ef0-0970-414e-955e-dc8c0a527b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved → /rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/models/fine_tuned/GoCLIP/features_lung.npz\n"
     ]
    }
   ],
   "source": [
    "save_npz(CFG.out_npz, cell_ids, v_raw, v_proj, t_raw, t_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed10fa64-0f80-466e-befb-bc751c538c41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phacosta (py3.10.12)",
   "language": "python",
   "name": "phacosta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
