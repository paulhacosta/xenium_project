{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9376ec80-9477-4f1b-9b05-b0af736a2f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contrastive training (CLIP‑style) with a UNI2 image encoder **and** the\n",
    "**CoCa text encoder** from *OpenCLIP* (matching OmiCLIP’s causal masking\n",
    "transformer).\n",
    "\n",
    "Key points\n",
    "==========\n",
    "* **Image branch**  – UNI2 model that pools the 4 centre spatial tokens\n",
    "  per cell.\n",
    "* **Text branch**   – CoCa causal text encoder (pre‑trained on LAION‑2B)\n",
    "  loaded via *open_clip*.\n",
    "* **Projection heads** map both modalities → shared dim (`proj_dim`).\n",
    "* **InfoNCE loss**   – symmetric (image→text & text→image).\n",
    "\n",
    "Dependencies\n",
    "------------\n",
    "```bash\n",
    "pip install open_clip_torch transformers timm opencv-python pillow\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from typing import List\n",
    "import timm\n",
    "import pandas as pd \n",
    "import argparse \n",
    "import open_clip           # ⇦ CoCa lives here\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Optional\n",
    "from PIL import Image\n",
    "import openslide\n",
    "import scanpy as sc\n",
    "import glob\n",
    "import math\n",
    "import csv, time, math\n",
    "from contextlib import nullcontext\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "181fbe52-2d4b-4693-8074-2dbb86cca7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Argparse → CFG\n",
    "# -----------------------------------------------------------------------------\n",
    "# parser = argparse.ArgumentParser(description=\"CLIP-GO contrastive training (UNI2 + CoCa)\")\n",
    "# parser.add_argument(\"--cancer\", type=str, default=\"lung\",\n",
    "#                     choices=[\"lung\",\"breast\",\"lymph_node\",\"prostate\",\"skin\",\"ovarian\",\"cervical\"]) \n",
    "# parser.add_argument(\"--epochs\", type=int, default=10)\n",
    "# parser.add_argument(\"--run_name\", type=str, default=\"clipgo_run\")\n",
    "# parser.add_argument(\"--freeze_text\", action=\"store_true\")\n",
    "# parser.add_argument(\"--batch_size\", type=int, default=72)\n",
    "# parser.add_argument(\"--num_workers\", type=int, default=8)\n",
    "# parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
    "# parser.add_argument(\"--weight_decay\", type=float, default=1e-3)\n",
    "# parser.add_argument(\"--proj_dim\", type=int, default=256)\n",
    "# parser.add_argument(\"--dropout\", type=float, default=0.1)\n",
    "# parser.add_argument(\"--context_len\", type=int, default=76)\n",
    "# parser.add_argument(\"--target_mpp\", type=float, default=0.5, help=\"target µm/px (≈20x)\")\n",
    "# parser.add_argument(\"--go_sentence_col\", type=str, default=\"go_sentence\")\n",
    "# args = parser.parse_args() if __name__ != \"__main__\" else parser.parse_args([])\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    # data\n",
    "    cancer = \"lung\"\n",
    "    ground_truth = \"refined\"            # dataset variant\n",
    "    level = 0                            # UNI2 spatial‑token level\n",
    "    batch_size = 72\n",
    "    num_workers = 8\n",
    "\n",
    "    # optimisation\n",
    "    temperature = 1.0\n",
    "    patience = 2.0\n",
    "    projection_dim = 256\n",
    "    lr = 1e-4\n",
    "    weight_decay = 1e-3\n",
    "    dropout = 0.1\n",
    "    epochs = 50\n",
    "\n",
    "    # Embeddings\n",
    "    morph_emb_dims = 1536\n",
    "    patch_size = 224\n",
    "\n",
    "    # Text / CoCa\n",
    "    coca_model = \"coca_ViT-L-14\"\n",
    "    coca_pretrain = \"laion2B-s13b-b90k\"\n",
    "    context_len =76\n",
    "    freeze_text = True\n",
    "    # go_sentence_col = args.go_sentence_col\n",
    "\n",
    "    # paths\n",
    "    root = \"/rsrch9/home/plm/idso_fa1_pathology/TIER1/paul-xenium/public_data/10x_genomics\"\n",
    "    xenium_sample_dict = {\n",
    "        \"lung\":\"Xenium_Prime_Human_Lung_Cancer_FFPE_outs\",\n",
    "        \"breast\": \"Xenium_Prime_Breast_Cancer_FFPE_outs\",\n",
    "        \"lymph_node\": \"Xenium_Prime_Human_Lymph_Node_Reactive_FFPE_outs\",\n",
    "        \"prostate\": \"Xenium_Prime_Human_Prostate_FFPE_outs\",\n",
    "        \"skin\": \"Xenium_Prime_Human_Skin_FFPE_outs\",\n",
    "        \"ovarian\": \"Xenium_Prime_Ovarian_Cancer_FFPE_outs\",\n",
    "        \"cervical\": \"Xenium_Prime_Cervical_Cancer_FFPE_outs\",\n",
    "    }\n",
    "    go_dir = \"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/gene_ontology\"\n",
    "    model_dir = \"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/models/public/UNI2-h\"  # pretrained UNI2 weights\n",
    "    ckpt_dir = f\"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/models/fine_tuned/GoCLIP/{cancer}\"  # outputs\n",
    "    target_mpp = 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc6aa4b9-09ef-4507-90a6-94f947c79eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Projection head\n",
    "# -----------------------------------------------------------------------------\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim: int, proj_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, proj_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(proj_dim, proj_dim)\n",
    "        self.ln  = nn.LayerNorm(proj_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.fc1(x)\n",
    "        x = self.act(h)\n",
    "        x = self.fc2(x)\n",
    "        return self.ln(x + h)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# UNI2 wrapper (4-centre token pooling)\n",
    "# -----------------------------------------------------------------------------\n",
    "class UNI2Wrapper(nn.Module):\n",
    "    def __init__(self, uni2: nn.Module, centre_idx: List[int]):\n",
    "        super().__init__()\n",
    "        self.uni2 = uni2\n",
    "        self.centre_idx = torch.tensor(centre_idx, dtype=torch.long)\n",
    "        self.prefix_tokens = 9\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        tok = self.uni2.forward_features(x)               # (B, 265, 1536)\n",
    "        spatial = tok[:, self.prefix_tokens :, :]\n",
    "        centre  = spatial.index_select(1, self.centre_idx.to(x.device)).mean(1)\n",
    "        return centre                                     # (B,1536)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Dataset: cell-centred patch + GO sentence\n",
    "# -----------------------------------------------------------------------------\n",
    "class CellPatchTextDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        slide,                        # OpenSlide object with read_region\n",
    "        cell_df: pd.DataFrame,        # index=cell_id, has x_centroid, y_centroid\n",
    "        sentences: pd.Series,         # index=cell_id, value=string sentence\n",
    "        transform,                    # torchvision transforms\n",
    "        scale: float,                 # px-per-px scaling for target magnification\n",
    "        patch_size: int = 224,\n",
    "    ):\n",
    "        self.slide = slide\n",
    "        self.cells = cell_df.reset_index(drop=False)   # keep cell_id in column 'index'\n",
    "        self.sentences = sentences\n",
    "        self.tfm = transform\n",
    "        self.scale = scale\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cells)\n",
    "\n",
    "    def _read_patch(self, x, y):\n",
    "        big = int(self.patch_size * self.scale)\n",
    "        tlx, tly = int(x - big/2), int(y - big/2)\n",
    "        patch = self.slide.read_region((tlx, tly), 0, (big, big)).convert(\"RGB\")\n",
    "        return patch.resize((self.patch_size, self.patch_size), Image.LANCZOS)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row   = self.cells.iloc[idx]\n",
    "        patch = self._read_patch(row.x_centroid, row.y_centroid)\n",
    "        img_t = self.tfm(patch)\n",
    "        cell_id = row[\"index\"]\n",
    "        sent = self.sentences.loc[cell_id]\n",
    "        sent = \"\" if pd.isna(sent) else str(sent)\n",
    "        return {\"image\": img_t, \"text\": sent}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CLIP-GO: UNI2 + CoCa text encoder\n",
    "# -----------------------------------------------------------------------------\n",
    "class CLIPGO(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_backbone: nn.Module,\n",
    "        coca_model: str = CFG.coca_model,\n",
    "        coca_pretrain: str = CFG.coca_pretrain,\n",
    "        proj_dim: int = CFG.projection_dim,\n",
    "        context_len: int = CFG.context_len,\n",
    "        freeze_text: bool = CFG.freeze_text,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.context_len = context_len\n",
    "        self.freeze_text = freeze_text\n",
    "\n",
    "        # ---- Vision branch ----\n",
    "        self.vision_encoder = vision_backbone\n",
    "        vision_dim = vision_backbone.uni2.embed_dim  # 1536 for ViT-Giant\n",
    "        self.vision_proj  = ProjectionHead(vision_dim, proj_dim)\n",
    "\n",
    "        # ---- Text branch (CoCa causal encoder) ----\n",
    "        self.text_encoder, _, _ = open_clip.create_model_and_transforms(\n",
    "            coca_model, pretrained=coca_pretrain,\n",
    "            cache_dir=os.path.expanduser(\"~/.cache/open_clip\")\n",
    "        )\n",
    "        self.tokenizer = open_clip.get_tokenizer(coca_model)\n",
    "        if freeze_text:\n",
    "            for p in self.text_encoder.parameters():\n",
    "                p.requires_grad_(False)\n",
    "\n",
    "        # Determine text width dynamically\n",
    "        with torch.no_grad():\n",
    "            dummy = self.tokenizer([\"dummy\"], context_length=context_len)\n",
    "            txt_feat = self.text_encoder.encode_text(dummy)\n",
    "        text_dim = txt_feat.shape[-1]\n",
    "        self.text_proj = ProjectionHead(text_dim, proj_dim)\n",
    "\n",
    "        # Learnable temperature\n",
    "        self.logit_scale = nn.Parameter(torch.log(torch.tensor(1/0.07)))\n",
    "\n",
    "    def encode_image(self, imgs):\n",
    "        feats = self.vision_encoder(imgs)\n",
    "        return self.vision_proj(feats)\n",
    "\n",
    "    def encode_text(self, sentences: List[str]):\n",
    "        tokens = self.tokenizer(sentences, context_length=self.context_len).to(next(self.parameters()).device)\n",
    "        if self.freeze_text:\n",
    "            with torch.no_grad():\n",
    "                feats = self.text_encoder.encode_text(tokens)\n",
    "        else:\n",
    "            feats = self.text_encoder.encode_text(tokens)\n",
    "        return self.text_proj(feats)\n",
    "\n",
    "    def forward(self, imgs, sentences):\n",
    "        img_emb = F.normalize(self.encode_image(imgs), dim=-1)\n",
    "        txt_emb = F.normalize(self.encode_text(sentences), dim=-1)\n",
    "        scale   = self.logit_scale.exp()\n",
    "        logits  = scale * img_emb @ txt_emb.T\n",
    "\n",
    "        targets = torch.arange(img_emb.size(0), device=img_emb.device)\n",
    "        loss_i = F.cross_entropy(logits, targets)\n",
    "        loss_t = F.cross_entropy(logits.T, targets)\n",
    "        return 0.5 * (loss_i + loss_t), logits, logits.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "235e54a4-eef4-42a2-b6b4-081e2ce5c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Helpers: slide/MPP, transforms, config dump\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def get_slide_and_mpp(slide_dir: str):\n",
    "    # try common names: *_he_image_registered.ome.tif inside sample folder\n",
    "    tifs = sorted(glob.glob(os.path.join(slide_dir, \"**\", \"*he_image_registered*.ome.tif\"), recursive=True))\n",
    "    if not tifs:\n",
    "        # fallback: any .tif in root\n",
    "        tifs = sorted(glob.glob(os.path.join(slide_dir, \"**\", \"*.tif\"), recursive=True))\n",
    "        assert tifs, f\"No slide tif found under {slide_dir}\"\n",
    "    slide_path = tifs[0]\n",
    "    slide = openslide.open_slide(slide_path)\n",
    "\n",
    "    # mpp search (robust):\n",
    "    props = slide.properties\n",
    "    mpp = None\n",
    "    for key in (\"openslide.mpp-x\", \"aperio.MPP\", \"tiff.XResolution\" ):\n",
    "        if key in props:\n",
    "            try:\n",
    "                mpp = float(props[key])\n",
    "                break\n",
    "            except Exception:\n",
    "                pass\n",
    "    if mpp is None:\n",
    "        # 10x fallback ≈ 0.5 µm/px\n",
    "        mpp = CFG.target_mpp\n",
    "    return slide, mpp, slide_path\n",
    "\n",
    "\n",
    "def build_transforms():\n",
    "    import torchvision.transforms as T\n",
    "    return T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "    ])\n",
    "\n",
    "\n",
    "def dump_cfg(path: str):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.safe_dump({k: v for k, v in vars(CFG).items() if not k.startswith(\"__\")}, f, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12c1fc4a-ca6e-41a9-9bd6-35674d8ec2b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Training\n",
    "# -----------------------------------------------------------------------------\n",
    "@torch.inference_mode()\n",
    "def batch_acc(logits):\n",
    "    target = torch.arange(logits.size(0), device=logits.device)\n",
    "    pred_i = logits.max(dim=1).indices\n",
    "    pred_t = logits.max(dim=0).indices\n",
    "    acc_i = (pred_i == target).float().mean()\n",
    "    acc_t = (pred_t == target).float().mean()\n",
    "    return acc_i.item(), acc_t.item()\n",
    "\n",
    "\n",
    "def train_clipgo(model: CLIPGO, loader: DataLoader, val_loader: DataLoader = None,\n",
    "                 grad_clip: float = 1.0, amp: bool = True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "\n",
    "    os.makedirs(CFG.ckpt_dir, exist_ok=True)\n",
    "    log_csv = os.path.join(CFG.ckpt_dir, \"train_log.csv\")\n",
    "    if not os.path.isfile(log_csv):\n",
    "        with open(log_csv, \"w\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow([\"epoch\",\"step\",\"loss\",\"acc_i\",\"acc_t\",\"lr\",\"logit_scale\",\"imgs_per_sec\"])\n",
    "\n",
    "    best = math.inf\n",
    "\n",
    "    for epoch in range(1, CFG.epochs + 1):\n",
    "        model.train()\n",
    "        running, ema_loss = 0.0, None\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        # Throughput helpers\n",
    "        seen = 0\n",
    "        step_start = time.time()\n",
    "\n",
    "        pbar = tqdm(loader, desc=f\"Epoch {epoch}/{CFG.epochs}\", dynamic_ncols=True)\n",
    "        for step, batch in enumerate(pbar, 1):\n",
    "            imgs  = batch[\"image\"].to(device, non_blocking=True)\n",
    "            texts = batch[\"text\"]\n",
    "            B = imgs.size(0)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            autocast_ctx = torch.cuda.amp.autocast(enabled=amp) if device.type == \"cuda\" else nullcontext()\n",
    "\n",
    "            try:\n",
    "                with autocast_ctx:\n",
    "                    loss, logits_it, logits_ti = model(imgs, texts)\n",
    "                scaler.scale(loss).backward()\n",
    "                if grad_clip is not None:\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                if device.type == \"cuda\":\n",
    "                    torch.cuda.empty_cache()\n",
    "                print(\"[WARN] CUDA OOM: skipping batch\")\n",
    "                continue\n",
    "\n",
    "            running += loss.item()\n",
    "            ema_loss = loss.item() if ema_loss is None else 0.9*ema_loss + 0.1*loss.item()\n",
    "            acc_i, acc_t = batch_acc(logits_it)\n",
    "\n",
    "            # throughput\n",
    "            seen += B\n",
    "            dt = max(time.time() - step_start, 1e-6)\n",
    "            ips = B / dt\n",
    "            step_start = time.time()\n",
    "\n",
    "            # current lr (first group)\n",
    "            lr = opt.param_groups[0][\"lr\"]\n",
    "            logit_scale = float(model.logit_scale.exp().detach().cpu())\n",
    "\n",
    "            pbar.set_postfix(loss=f\"{ema_loss:.4f}\", acc_i=f\"{acc_i:.3f}\",\n",
    "                             acc_t=f\"{acc_t:.3f}\", lr=f\"{lr:.1e}\",\n",
    "                             τ=f\"{1.0/logit_scale:.3f}\", ips=f\"{ips:.0f}\")\n",
    "\n",
    "            # CSV log (every ~50 steps)\n",
    "            if step % 50 == 0 or step == len(loader):\n",
    "                with open(log_csv, \"a\", newline=\"\") as f:\n",
    "                    csv.writer(f).writerow([epoch, step, loss.item(), acc_i, acc_t, lr, logit_scale, ips])\n",
    "\n",
    "        # ----- epoch end -----\n",
    "        avg = running / max(len(loader), 1)\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"Epoch {epoch:03d} | loss {avg:.4f} | time {epoch_time/60:.1f} min | \"\n",
    "              f\"logit_scale {logit_scale:.3f} (τ≈{1.0/logit_scale:.3f})\")\n",
    "\n",
    "        # checkpoints\n",
    "        torch.save({\"epoch\": epoch, \"model\": model.state_dict()},\n",
    "                   os.path.join(CFG.ckpt_dir, f\"epoch_{epoch:03d}.pth\"))\n",
    "        torch.save({\"epoch\": epoch, \"model\": model.state_dict()},\n",
    "                   os.path.join(CFG.ckpt_dir, \"last.pth\"))\n",
    "        if avg < best:\n",
    "            best = avg\n",
    "            torch.save({\"epoch\": epoch, \"model\": model.state_dict()},\n",
    "                       os.path.join(CFG.ckpt_dir, \"best.pth\"))\n",
    "            print(\"✓ new best\")\n",
    "\n",
    "    dump_cfg(os.path.join(CFG.ckpt_dir, \"config.yaml\"))\n",
    "    print(\"Training complete. Best loss:\", best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28f4dfa-01db-41a9-b023-b2ac48e99dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 3398/3398 [22:24<00:00,  2.53it/s, acc_i=0.153, acc_t=0.097, ips=220, loss=3.4355, lr=1.0e-04, τ=0.068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | loss 3.5743 | time 22.4 min | logit_scale 14.633 (τ≈0.068)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 3398/3398 [21:31<00:00,  2.63it/s, acc_i=0.194, acc_t=0.222, ips=224, loss=3.3200, lr=1.0e-04, τ=0.066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 | loss 3.3916 | time 21.5 min | logit_scale 15.090 (τ≈0.066)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 3398/3398 [22:31<00:00,  2.51it/s, acc_i=0.125, acc_t=0.153, ips=226, loss=3.3351, lr=1.0e-04, τ=0.064] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 | loss 3.2885 | time 22.5 min | logit_scale 15.626 (τ≈0.064)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 3398/3398 [21:37<00:00,  2.62it/s, acc_i=0.153, acc_t=0.208, ips=226, loss=3.1627, lr=1.0e-04, τ=0.061] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 | loss 3.1783 | time 21.6 min | logit_scale 16.336 (τ≈0.061)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 3398/3398 [22:02<00:00,  2.57it/s, acc_i=0.222, acc_t=0.236, ips=225, loss=3.1382, lr=1.0e-04, τ=0.057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 | loss 3.0321 | time 22.0 min | logit_scale 17.460 (τ≈0.057)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 3398/3398 [21:35<00:00,  2.62it/s, acc_i=0.236, acc_t=0.292, ips=227, loss=2.8000, lr=1.0e-04, τ=0.052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006 | loss 2.7805 | time 21.6 min | logit_scale 19.354 (τ≈0.052)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 3398/3398 [22:11<00:00,  2.55it/s, acc_i=0.292, acc_t=0.306, ips=229, loss=2.3808, lr=1.0e-04, τ=0.043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 | loss 2.3386 | time 22.2 min | logit_scale 23.008 (τ≈0.043)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 3398/3398 [21:25<00:00,  2.64it/s, acc_i=0.403, acc_t=0.389, ips=227, loss=2.0970, lr=1.0e-04, τ=0.036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008 | loss 1.8095 | time 21.4 min | logit_scale 27.603 (τ≈0.036)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 3398/3398 [22:01<00:00,  2.57it/s, acc_i=0.569, acc_t=0.611, ips=228, loss=1.2340, lr=1.0e-04, τ=0.029]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009 | loss 0.9870 | time 22.0 min | logit_scale 34.260 (τ≈0.029)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 3398/3398 [21:29<00:00,  2.64it/s, acc_i=0.792, acc_t=0.778, ips=228, loss=0.6872, lr=1.0e-04, τ=0.024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 | loss 0.5523 | time 21.5 min | logit_scale 41.662 (τ≈0.024)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 3398/3398 [21:46<00:00,  2.60it/s, acc_i=0.889, acc_t=0.931, ips=228, loss=0.2894, lr=1.0e-04, τ=0.020]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011 | loss 0.3049 | time 21.8 min | logit_scale 49.302 (τ≈0.020)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 3398/3398 [21:24<00:00,  2.65it/s, acc_i=0.931, acc_t=0.931, ips=224, loss=0.2590, lr=1.0e-04, τ=0.019]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012 | loss 0.2946 | time 21.4 min | logit_scale 52.688 (τ≈0.019)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 3398/3398 [22:16<00:00,  2.54it/s, acc_i=0.861, acc_t=0.889, ips=228, loss=0.2686, lr=1.0e-04, τ=0.017]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013 | loss 0.1750 | time 22.3 min | logit_scale 57.727 (τ≈0.017)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 3398/3398 [22:12<00:00,  2.55it/s, acc_i=0.931, acc_t=0.931, ips=228, loss=0.1612, lr=1.0e-04, τ=0.017] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014 | loss 0.2281 | time 22.2 min | logit_scale 59.263 (τ≈0.017)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 3398/3398 [21:58<00:00,  2.58it/s, acc_i=0.958, acc_t=0.931, ips=228, loss=0.1314, lr=1.0e-04, τ=0.016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015 | loss 0.1266 | time 22.0 min | logit_scale 63.052 (τ≈0.016)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 3398/3398 [21:37<00:00,  2.62it/s, acc_i=0.917, acc_t=0.931, ips=228, loss=0.1242, lr=1.0e-04, τ=0.015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016 | loss 0.1137 | time 21.6 min | logit_scale 65.670 (τ≈0.015)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 3398/3398 [21:39<00:00,  2.62it/s, acc_i=0.986, acc_t=0.944, ips=226, loss=0.1138, lr=1.0e-04, τ=0.015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017 | loss 0.1029 | time 21.7 min | logit_scale 67.658 (τ≈0.015)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 3398/3398 [22:07<00:00,  2.56it/s, acc_i=0.958, acc_t=0.958, ips=227, loss=0.1070, lr=1.0e-04, τ=0.014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018 | loss 0.0947 | time 22.1 min | logit_scale 69.055 (τ≈0.014)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 3398/3398 [21:26<00:00,  2.64it/s, acc_i=0.958, acc_t=0.944, ips=227, loss=0.1086, lr=1.0e-04, τ=0.014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 019 | loss 0.0899 | time 21.4 min | logit_scale 70.037 (τ≈0.014)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 3398/3398 [21:26<00:00,  2.64it/s, acc_i=1.000, acc_t=0.986, ips=227, loss=0.0870, lr=1.0e-04, τ=0.014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 | loss 0.0748 | time 21.4 min | logit_scale 71.829 (τ≈0.014)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 3398/3398 [21:18<00:00,  2.66it/s, acc_i=0.972, acc_t=0.958, ips=229, loss=0.0897, lr=1.0e-04, τ=0.014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 021 | loss 0.0797 | time 21.3 min | logit_scale 72.083 (τ≈0.014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 3398/3398 [21:07<00:00,  2.68it/s, acc_i=0.944, acc_t=0.931, ips=226, loss=0.0926, lr=1.0e-04, τ=0.014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 022 | loss 0.0742 | time 21.1 min | logit_scale 72.651 (τ≈0.014)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 3398/3398 [18:33<00:00,  3.05it/s, acc_i=0.972, acc_t=1.000, ips=228, loss=0.0672, lr=1.0e-04, τ=0.014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 023 | loss 0.0708 | time 18.6 min | logit_scale 73.367 (τ≈0.014)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 3398/3398 [18:26<00:00,  3.07it/s, acc_i=0.986, acc_t=0.986, ips=228, loss=0.0539, lr=1.0e-04, τ=0.014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 024 | loss 0.0681 | time 18.4 min | logit_scale 73.637 (τ≈0.014)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 3398/3398 [18:18<00:00,  3.09it/s, acc_i=1.000, acc_t=0.972, ips=230, loss=0.0579, lr=1.0e-04, τ=0.013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 025 | loss 0.0628 | time 18.3 min | logit_scale 74.123 (τ≈0.013)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 3398/3398 [18:23<00:00,  3.08it/s, acc_i=0.972, acc_t=0.903, ips=227, loss=0.0822, lr=1.0e-04, τ=0.013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 026 | loss 0.0641 | time 18.4 min | logit_scale 74.279 (τ≈0.013)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 3398/3398 [18:31<00:00,  3.06it/s, acc_i=0.972, acc_t=0.958, ips=229, loss=0.0550, lr=1.0e-04, τ=0.013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 027 | loss 0.0615 | time 18.5 min | logit_scale 75.060 (τ≈0.013)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 3398/3398 [18:44<00:00,  3.02it/s, acc_i=0.986, acc_t=0.972, ips=226, loss=0.0739, lr=1.0e-04, τ=0.013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 028 | loss 0.0598 | time 18.7 min | logit_scale 75.427 (τ≈0.013)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 3398/3398 [18:51<00:00,  3.00it/s, acc_i=0.986, acc_t=0.986, ips=227, loss=0.0531, lr=1.0e-04, τ=0.013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 029 | loss 0.0569 | time 18.9 min | logit_scale 75.823 (τ≈0.013)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 3398/3398 [18:38<00:00,  3.04it/s, acc_i=0.986, acc_t=0.986, ips=228, loss=0.0558, lr=1.0e-04, τ=0.013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 030 | loss 0.0534 | time 18.6 min | logit_scale 77.017 (τ≈0.013)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 3398/3398 [19:07<00:00,  2.96it/s, acc_i=0.986, acc_t=0.986, ips=227, loss=0.0513, lr=1.0e-04, τ=0.013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 031 | loss 0.0507 | time 19.1 min | logit_scale 77.350 (τ≈0.013)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 3398/3398 [19:42<00:00,  2.87it/s, acc_i=0.972, acc_t=0.958, ips=229, loss=0.0530, lr=1.0e-04, τ=0.013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 032 | loss 0.0486 | time 19.7 min | logit_scale 77.923 (τ≈0.013)\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50:  11%|█         | 380/3398 [02:13<16:45,  3.00it/s, acc_i=1.000, acc_t=0.986, ips=227, loss=0.0346, lr=1.0e-04, τ=0.013]"
     ]
    }
   ],
   "source": [
    "# Resolve dataset-specific paths\n",
    "sample  = CFG.xenium_sample_dict[CFG.cancer]\n",
    "sample_dir = os.path.join(CFG.root, sample)\n",
    "\n",
    "# Load AnnData (expects x_centroid / y_centroid in .obs)\n",
    "adata_path = os.path.join(\n",
    "    sample_dir,\n",
    "    \"preprocessed\",\n",
    "    f\"fine_tune_{CFG.ground_truth}_v2\",\n",
    "    f\"processed_xenium_data_fine_tune_{CFG.ground_truth}_v2_annotated.h5ad\",\n",
    ")\n",
    "adata = sc.read_h5ad(adata_path)\n",
    "cell_df = adata.obs.copy()  # index = cell_id\n",
    "\n",
    "# Sentences: external file (index=cell_id, column=go_sentences)\n",
    "sentences = pd.read_csv(f\"{CFG.go_dir}/{sample.replace('outs', 'GO.csv')}\", index_col=\"cell_id\")\n",
    "sentences = sentences[\"go_sentences\"].astype(str)       # ensure it's a Series\n",
    "sentences = sentences.reindex(cell_df.index)            # align to adata\n",
    "missing = sentences.isna().sum()\n",
    "if missing > 0:\n",
    "    print(f\"[WARN] {missing} cells missing sentences; filling with empty strings.\")\n",
    "    sentences = sentences.fillna(\"\")\n",
    "    \n",
    "# Slide + scale\n",
    "slide, mpp, slide_path = get_slide_and_mpp(sample_dir)\n",
    "scale_factor = max(CFG.target_mpp / float(mpp), 1e-6)\n",
    "\n",
    "# Build vision backbone (UNI2)\n",
    "uni2_cfg = {\n",
    "    'model_name':'vit_giant_patch14_224','img_size':224,'patch_size':14,'depth':24,\n",
    "    'num_heads':24,'init_values':1e-5,'embed_dim':1536,'mlp_ratio':2.66667*2,\n",
    "    'num_classes':0,'no_embed_class':True,'mlp_layer':timm.layers.SwiGLUPacked,\n",
    "    'act_layer':torch.nn.SiLU,'reg_tokens':8,'dynamic_img_size':True,\n",
    "}\n",
    "\n",
    "uni2 = timm.create_model(pretrained=False, **uni2_cfg)\n",
    "# Load weights if you have them\n",
    "uni2_weights = os.path.join(CFG.model_dir, \"pytorch_model.bin\")\n",
    "uni2.load_state_dict(torch.load(os.path.join(uni2_weights), map_location=\"cpu\"), strict=True)\n",
    "\n",
    "\n",
    "centre_idx = [119, 120, 135, 136] if CFG.level == 0 else [\n",
    "    102,103,104,105,118,119,120,121,134,135,136,137,150,151,152,153\n",
    "]\n",
    "vision_backbone = UNI2Wrapper(uni2, centre_idx)\n",
    "\n",
    "# Dataset / DataLoader\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "])\n",
    "# DataLoader – consider drop_last=True for strict CLIP pairing\n",
    "dataset = CellPatchTextDataset(slide, cell_df, sentences, transform,\n",
    "                               scale=scale_factor, patch_size=CFG.patch_size)\n",
    "loader  = DataLoader(dataset,\n",
    "                     batch_size=CFG.batch_size,\n",
    "                     shuffle=True,\n",
    "                     num_workers=CFG.num_workers,\n",
    "                     pin_memory=True,\n",
    "                     persistent_workers=True,\n",
    "                     prefetch_factor=4,\n",
    "                     drop_last=True)   # <- recommended for contrastive matching\n",
    "\n",
    "# Model + train\n",
    "model = CLIPGO(vision_backbone)\n",
    "os.makedirs(CFG.ckpt_dir, exist_ok=True)\n",
    "train_clipgo(model, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9fddbf-7949-4da7-b629-24b62828e09c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7085de-9a85-45ec-bb2e-d8041d887cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573af413-9c74-4484-9e87-00eb093c7d91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phacosta (py3.10.12)",
   "language": "python",
   "name": "phacosta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
