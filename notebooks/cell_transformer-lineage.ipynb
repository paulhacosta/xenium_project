{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525d8922-9b0a-491b-91cb-bc878ecd3646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scanpy as sc \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11b6a08d-5eca-439f-958b-c32beb8aea72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using granular labels from SingleR.\n",
      "Using 11 valid markers: ['CD3E', 'CD3G', 'CD4', 'CD8A', 'CD8B', 'CD19', 'CD27', 'CD68', 'PDGFRA', 'EPCAM', 'EGFR']\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "# Data params\n",
    "platform = \"xenium\" # xenium or visium \n",
    "ground_truth = \"refined\"  # refined or cellvit\n",
    "level = 0\n",
    "filtered_genes = False\n",
    "granular_labels = True\n",
    "use_singleR_qc = True\n",
    "limit_classes = True  # Set to False to use all classes\n",
    "\n",
    "# Training params\n",
    "use_focal_loss = False\n",
    "\n",
    "\n",
    "if platform == \"xenium\":\n",
    "    cancer = \"lung\"\n",
    "    xenium_folder_dict = {\"lung\": \"Xenium_Prime_Human_Lung_Cancer_FFPE_outs\",\n",
    "                          \"breast\":\"Xenium_Prime_Breast_Cancer_FFPE_outs\",\n",
    "                          \"lymph_node\": \"Xenium_Prime_Human_Lymph_Node_Reactive_FFPE_outs\",\n",
    "                          \"prostate\": \"Xenium_Prime_Human_Prostate_FFPE_outs\",\n",
    "                          \"skin\": \"Xenium_Prime_Human_Skin_FFPE_outs\",\n",
    "                          \"ovarian\": \"Xenium_Prime_Ovarian_Cancer_FFPE_outs\",\n",
    "                          \"cervical\": \"Xenium_Prime_Cervical_Cancer_FFPE_outs\"\n",
    "                          }\n",
    "\n",
    "    xenium_folder = xenium_folder_dict[cancer]\n",
    "    \n",
    "    data_path = f\"/rsrch9/home/plm/idso_fa1_pathology/TIER1/paul-xenium/public_data/10x_genomics/{xenium_folder}/preprocessed/fine_tune_{ground_truth}_v2/processed_xenium_data_fine_tune_{ground_truth}_v2_annotated.h5ad\"\n",
    "    # data_path = \"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/embeddings/public_data/Xenium_Prime_Human_Lung_Cancer_FFPE_outs/scGPT_CP.h5ad\"\n",
    "\n",
    "    if filtered_genes:\n",
    "        gene_embedding_file = f\"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/embeddings/public_data/{xenium_folder}/processed_xenium_refined_clustering_filtered_v2.csv\"\n",
    "    else:\n",
    "        gene_embedding_file = f\"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/embeddings/public_data/{xenium_folder}/processed_xenium_{ground_truth}_v2.csv\"\n",
    "    \n",
    "    # Load Morphological Embeddings\n",
    "    morph_embedding_dir = \"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/embeddings/public_data/Xenium_Prime_Human_Lung_Cancer_FFPE_outs\"\n",
    "\n",
    "    # Load Morphological Embeddings\n",
    "    morph_embedding_dir = f\"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/embeddings/public_data/{xenium_folder}\"\n",
    "\n",
    "        \n",
    "elif platform == \"visium\":\n",
    "    data_path = \"/rsrch9/home/plm/idso_fa1_pathology/TIER1/paul-xenium/public_data/10x_genomics/Visium_HD_Human_Lung_Cancer_post_Xenium_Prime_5k_Experiment2/binned_outputs/square_002um/preprocessed/bin2cell/to_tokenize/corrected_cells_matched_preprocessed_refined_v2_annotated.h5ad\"\n",
    "\n",
    "    gene_embedding_file = \"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/embeddings/public_data/Visium_HD_Human_Lung_Cancer_post_Xenium_Prime_5k_Experiment2/bin2cell/embeddings_output/processed_visium_hd_bin2cell.csv\"\n",
    "    morph_embedding_dir = f\"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/embeddings/public_data/Visium_HD_Human_Lung_Cancer_post_Xenium_Prime_5k_Experiment2\"\n",
    "\n",
    "# Load AnnData\n",
    "adata = sc.read_h5ad(data_path)\n",
    "cell_data = adata.obs\n",
    "\n",
    "# Spatial Information \n",
    "spatial_coords = cell_data[['x_centroid', 'y_centroid']].rename(columns={'x_centroid': 'x', 'y_centroid': 'y'})\n",
    "\n",
    "# Load gene Embeddings \n",
    "# gene_embeddings = pd.read_csv(gene_embedding_file, index_col=\"Unnamed: 0\")\n",
    "# gene_embeddings.index = cell_data.index\n",
    "scGPT_path = \"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/embeddings/public_data/Xenium_Prime_Human_Lung_Cancer_FFPE_outs/scGPT_CP.h5ad\"\n",
    "scGPT_adata = sc.read_h5ad(scGPT_path)\n",
    "gene_embeddings = pd.DataFrame(scGPT_adata.obsm[\"X_scGPT\"])\n",
    "gene_embeddings.index = cell_data.index\n",
    "\n",
    "# Load Morphology Embeddings \n",
    "morph_embedding_csv = os.path.join(morph_embedding_dir, \"UNI2_cell_representation\",f\"level_{level}\",\"morphology_embeddings_v2.csv\")\n",
    "morph_embeddings = pd.read_csv(morph_embedding_csv, index_col=\"Unnamed: 0\")\n",
    "\n",
    "\n",
    "if granular_labels:\n",
    "    print(\"Using granular labels from SingleR.\")\n",
    "    label_key = \"granular_class\"\n",
    "    singleR_to_class_map = {\n",
    "        \"Smooth muscle\": \"fibroblast\",\n",
    "        \"Fibroblasts\": \"fibroblast\",\n",
    "        \"Endothelial cells\": \"fibroblast\",\n",
    "        \"CD4+ T-cells\": \"t_cell\",\n",
    "        \"CD8+ T-cells\": \"t_cell\",\n",
    "        \"B-cells\": \"b_cell\",\n",
    "        \"Macrophages\": \"macrophage\",\n",
    "        \"Epithelial cells\": \"tumor\",\n",
    "    }\n",
    "    \n",
    "    target_classes = [\"fibroblast\", #\"endothelial\",\n",
    "                      \"t_cell\", \"b_cell\", \"macrophage\",\n",
    "                      \"tumor\"]\n",
    "    \n",
    "    # Map SingleR labels to 7-class system\n",
    "    cell_data[label_key] = cell_data[\"singleR_class\"].map(singleR_to_class_map)\n",
    "    \n",
    "    # Drop cells that are nan (if any)\n",
    "    cell_data = cell_data.dropna(subset=[label_key])\n",
    "    \n",
    "    # Keep only those 7 classes\n",
    "    cell_data = cell_data[cell_data[label_key].isin(target_classes)]\n",
    "    \n",
    "    if use_singleR_qc:\n",
    "        cell_data = cell_data[cell_data[\"qc_singleR\"]==1]\n",
    "\n",
    "    \n",
    "    # Reindex embeddings/coords\n",
    "    gene_embeddings = gene_embeddings.reindex(cell_data.index)\n",
    "    morph_embeddings = morph_embeddings.reindex(cell_data.index)\n",
    "    spatial_coords = spatial_coords.reindex(cell_data.index)\n",
    "    \n",
    "    # Numeric mapping\n",
    "    label_mapping = {cls_name: i for i, cls_name in enumerate(target_classes)}\n",
    "    labels = pd.Series(cell_data[\"granular_class\"].map(label_mapping))\n",
    "    \n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Using AISTIL labels\")\n",
    "    label_key = \"class\"\n",
    "    target_classes = [\"f\", \"l\", \"t\"]  # Modify this list to restrict classification to specific classes\n",
    "    if limit_classes:\n",
    "        num_classes = len(target_classes)\n",
    "        cell_data = cell_data[cell_data[label_key].isin(target_classes)]\n",
    "\n",
    "        # Change index type for Visium data to match embeddings Idxs \n",
    "        if platform == \"visium\":\n",
    "            morph_embeddings.index = morph_embeddings.index.astype(str)\n",
    "\n",
    "        # Update corresponding embeddings and spatial coordinates\n",
    "        gene_embeddings = gene_embeddings.reindex(cell_data.index)\n",
    "        morph_embeddings = morph_embeddings.reindex(cell_data.index)\n",
    "        spatial_coords = spatial_coords.reindex(cell_data.index)\n",
    "    else:\n",
    "        target_classes = [\"f\",\"l\",\"o\",\"t\"]\n",
    "    \n",
    "# Convert labels to numerical indices\n",
    "# label_mapping = {l:c for c,l in enumerate(target_classes)}\n",
    "# labels = pd.Series([label_mapping[lbl] for lbl in cell_data[\"class\"]])\n",
    "\n",
    "num_classes = len(target_classes)\n",
    "label_mapping = {cls_name: i for i, cls_name in enumerate(target_classes)}\n",
    "labels = pd.Series(cell_data[label_key].map(label_mapping))\n",
    "\n",
    "\n",
    "marker_genes = [\n",
    "    \"CD3E\",   # T-cells\n",
    "    \"CD3G\",    # T-cells\n",
    "    \"CD4\",    # T-cells\n",
    "    \"CD8A\",    # T-cells\n",
    "    \"CD8B\",    # T-cells\n",
    "    \"CD19\",   # B-cells\n",
    "    \"CD27\",  # B-cells\n",
    "    \"CD68\",   # Macrophages\n",
    "    \"PDGFRA\", # stromal\n",
    "    \"EPCAM\",  # Epithelial / tumor\n",
    "    \"EGFR\",    # epithelial tumor\n",
    "    ]\n",
    "\n",
    "# Ensure adata.var_names are the gene names\n",
    "# Make sure marker_genes are present\n",
    "valid_markers = [g for g in marker_genes if g in adata.var_names]\n",
    "print(f\"Using {len(valid_markers)} valid markers: {valid_markers}\")\n",
    "\n",
    "# Subset the X matrix or a relevant layer\n",
    "# e.g., adata.raw.X or adata.X if everything is properly stored\n",
    "marker_expr = adata[:, valid_markers].X  # shape = (n_cells, num_markers)\n",
    "\n",
    "# Convert to a DataFrame for convenience, aligned with adata.obs\n",
    "marker_expr_df = pd.DataFrame(marker_expr.toarray() if hasattr(marker_expr, \"toarray\") else marker_expr,\n",
    "                              columns=valid_markers,\n",
    "                              index=adata.obs.index)\n",
    "# Suppose cell_data is filtered to your final set of cells\n",
    "marker_expr_df = marker_expr_df.reindex(cell_data.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58ee26a9-a77c-40fd-9767-5b1685fc30a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell data index: ['aaaaadnb-1', 'aaaabalp-1', 'aaaadjia-1', 'aaaafglb-1', 'aaaagbdd-1']\n",
      "Morph embeddings index: ['aaaaadnb-1', 'aaaabalp-1', 'aaaadjia-1', 'aaaafglb-1', 'aaaagbdd-1']\n",
      "Gene embeddings index: ['aaaaadnb-1', 'aaaabalp-1', 'aaaadjia-1', 'aaaafglb-1', 'aaaagbdd-1']\n",
      "Spatial coords index: ['aaaaadnb-1', 'aaaabalp-1', 'aaaadjia-1', 'aaaafglb-1', 'aaaagbdd-1']\n",
      "adata.X dtype: float32\n",
      "Min: 0.0 Max: 8.229777\n",
      "Mean: 0.18159501\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CD3E</th>\n",
       "      <th>CD3G</th>\n",
       "      <th>CD4</th>\n",
       "      <th>CD8A</th>\n",
       "      <th>CD8B</th>\n",
       "      <th>CD19</th>\n",
       "      <th>CD27</th>\n",
       "      <th>CD68</th>\n",
       "      <th>PDGFRA</th>\n",
       "      <th>EPCAM</th>\n",
       "      <th>EGFR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaaaadnb-1</th>\n",
       "      <td>4.895923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.812154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.812154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaabalp-1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaadjia-1</th>\n",
       "      <td>4.811788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.449600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaafglb-1</th>\n",
       "      <td>4.500041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.423402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaagbdd-1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oijmaenh-1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oijpapcb-1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.261694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oijpeago-1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oikbajbf-1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oikdmkje-1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122209 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                CD3E  CD3G       CD4      CD8A  CD8B  CD19  CD27      CD68  \\\n",
       "aaaaadnb-1  4.895923   0.0  3.812154  0.000000   0.0   0.0   0.0  3.812154   \n",
       "aaaabalp-1  0.000000   0.0  0.000000  0.000000   0.0   0.0   0.0  0.000000   \n",
       "aaaadjia-1  4.811788   0.0  3.449600  0.000000   0.0   0.0   0.0  0.000000   \n",
       "aaaafglb-1  4.500041   0.0  0.000000  3.423402   0.0   0.0   0.0  0.000000   \n",
       "aaaagbdd-1  0.000000   0.0  0.000000  0.000000   0.0   0.0   0.0  0.000000   \n",
       "...              ...   ...       ...       ...   ...   ...   ...       ...   \n",
       "oijmaenh-1  0.000000   0.0  0.000000  0.000000   0.0   0.0   0.0  0.000000   \n",
       "oijpapcb-1  0.000000   0.0  0.000000  0.000000   0.0   0.0   0.0  0.000000   \n",
       "oijpeago-1  0.000000   0.0  0.000000  0.000000   0.0   0.0   0.0  0.000000   \n",
       "oikbajbf-1  0.000000   0.0  0.000000  0.000000   0.0   0.0   0.0  0.000000   \n",
       "oikdmkje-1  0.000000   0.0  0.000000  0.000000   0.0   0.0   0.0  0.000000   \n",
       "\n",
       "              PDGFRA  EPCAM  EGFR  \n",
       "aaaaadnb-1  0.000000    0.0   0.0  \n",
       "aaaabalp-1  0.000000    0.0   0.0  \n",
       "aaaadjia-1  0.000000    0.0   0.0  \n",
       "aaaafglb-1  0.000000    0.0   0.0  \n",
       "aaaagbdd-1  0.000000    0.0   0.0  \n",
       "...              ...    ...   ...  \n",
       "oijmaenh-1  0.000000    0.0   0.0  \n",
       "oijpapcb-1  4.261694    0.0   0.0  \n",
       "oijpeago-1  0.000000    0.0   0.0  \n",
       "oikbajbf-1  0.000000    0.0   0.0  \n",
       "oikdmkje-1  0.000000    0.0   0.0  \n",
       "\n",
       "[122209 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Cell data index:\", cell_data.index.tolist()[:5])\n",
    "print(\"Morph embeddings index:\", morph_embeddings.index.tolist()[:5])\n",
    "print(\"Gene embeddings index:\", gene_embeddings.index.tolist()[:5])\n",
    "print(\"Spatial coords index:\", spatial_coords.index.tolist()[:5])\n",
    "print(\"adata.X dtype:\", adata.X.dtype)\n",
    "print(\"Min:\", adata.X.min(), \"Max:\", adata.X.max())\n",
    "print(\"Mean:\", adata.X.mean())\n",
    "marker_expr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602e1fda-62b5-4f42-8036-41977b482f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 1.2414 | Accuracy: 51.25%\n",
      "Test: Loss: 0.5802 | Accuracy: 83.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  69%|██████▉   | 1056/1528 [00:32<00:13, 34.69it/s, loss=0.334]"
     ]
    }
   ],
   "source": [
    "\n",
    "# focal Loss\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-class Focal Loss implementation\n",
    "    gamma: focusing parameter\n",
    "    alpha: can be a single float (scalar) or a Tensor of shape [num_classes]\n",
    "    reduction: 'mean' or 'sum'\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        # alpha can be scalar or per-class weights\n",
    "        # If alpha is a list/tuple/np array, convert it to a FloatTensor\n",
    "        if alpha is not None:\n",
    "            if isinstance(alpha, (float, int)):\n",
    "                self.alpha = torch.tensor([alpha], dtype=torch.float)\n",
    "            else:\n",
    "                self.alpha = torch.as_tensor(alpha, dtype=torch.float)\n",
    "        else:\n",
    "            self.alpha = None\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        inputs: [N, C] logits (no softmax)\n",
    "        targets: [N] class indices in [0, C-1]\n",
    "        \"\"\"\n",
    "        # Standard cross-entropy (per-sample)\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=None)\n",
    "        \n",
    "        # Convert to probabilities of correct class\n",
    "        pt = torch.exp(-ce_loss)  # = exp(-CE) = p_t\n",
    "\n",
    "        # If alpha is per-class, pick alpha for each target\n",
    "        if self.alpha is not None:\n",
    "            # If alpha is scalar, multiply entire loss by alpha\n",
    "            # If alpha is a tensor of shape [C], pick per-sample alpha\n",
    "            if len(self.alpha) == 1:\n",
    "                focal_loss = self.alpha[0] * ((1 - pt) ** self.gamma) * ce_loss\n",
    "            else:\n",
    "                # alpha for each class\n",
    "                alpha_t = self.alpha[targets]  # shape [N]\n",
    "                focal_loss = alpha_t * ((1 - pt) ** self.gamma) * ce_loss\n",
    "        else:\n",
    "            # No class weighting\n",
    "            focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        \n",
    "        # Reduction\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss  # 'none' for no reduction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding2D(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding for spatial coordinates\"\"\"\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.proj = nn.Linear(4, d_model)  # (sin(x), cos(x), sin(y), cos(y))\n",
    "        \n",
    "    def forward(self, coords):\n",
    "        # Ensure coords is [B, 2]\n",
    "        assert coords.ndim == 2, f\"Expected coords shape [B, 2], but got {coords.shape}\"\n",
    "        x = coords[:, 0] * 2 * torch.pi\n",
    "        y = coords[:, 1] * 2 * torch.pi\n",
    "        \n",
    "        pe = torch.stack([\n",
    "            torch.sin(x), torch.cos(x),\n",
    "            torch.sin(y), torch.cos(y)\n",
    "        ], dim=-1)  # [B, 4]\n",
    "        pe = self.proj(pe)  # [B, d_model]\n",
    "        return pe\n",
    "\n",
    "# Transformer Layer with Relative Position Attention\n",
    "class RelativePositionTransformerLayer(nn.TransformerEncoderLayer):\n",
    "    \"\"\"Enhanced with relative position attention\"\"\"\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        # Initialize parent class with batch_first=True.\n",
    "        super().__init__(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        self.pos_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.pos_norm = nn.LayerNorm(d_model)\n",
    "        # Add a dropout for the feedforward branch\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, pos_emb):\n",
    "        # Uncomment to debug\n",
    "        # print(f\"src shape before attn: {src.shape}\")   # Expected [B, 2, d_model]\n",
    "        # print(f\"pos_emb shape before attn: {pos_emb.shape}\")  # Expected [B, 1, d_model]\n",
    "\n",
    "        # Ensure pos_emb has the same sequence length as src.\n",
    "        pos_emb = pos_emb.expand(-1, src.shape[1], -1)  # Now [B, 2, d_model]\n",
    "        \n",
    "        # Uncomment to debug\n",
    "        # print(f\"pos_emb shape after expansion: {pos_emb.shape}\")  # Should be [B, 2, d_model]\n",
    "\n",
    "        # Standard self-attention\n",
    "        src2 = self.self_attn(src, src, src, need_weights=False)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "\n",
    "        # Position-aware attention\n",
    "        src2 = self.pos_attn(src, pos_emb, pos_emb)[0]\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.pos_norm(src)\n",
    "\n",
    "        # Feedforward\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout3(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class CellTransformer(nn.Module):\n",
    "    def __init__(self, d_model=512, num_heads=8, num_classes=3, num_markers=0):\n",
    "        super().__init__()\n",
    "\n",
    "        # Let's assume your Geneformer embedding is 512-D\n",
    "        # and you have num_markers columns for raw expression\n",
    "        self.gene_input_dim = 512 + num_markers\n",
    "\n",
    "        # MLP for combined gene+markers\n",
    "        self.gene_encoder = nn.Sequential(\n",
    "            nn.Linear(self.gene_input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, d_model)\n",
    "        )\n",
    "\n",
    "        # MLP for morphology\n",
    "        self.morph_encoder = nn.Sequential(\n",
    "            nn.Linear(1536, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, d_model)\n",
    "        )\n",
    "\n",
    "        self.spatial_pe = PositionalEncoding2D(d_model)\n",
    "        self.gene_type = nn.Parameter(torch.randn(1, d_model))\n",
    "        self.morph_type = nn.Parameter(torch.randn(1, d_model))\n",
    "\n",
    "        # 6 Transformer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            RelativePositionTransformerLayer(d_model, nhead=num_heads)\n",
    "            for _ in range(6)\n",
    "        ])\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, gene, morph, spatial, marker):\n",
    "        # gene: [B, 512]\n",
    "        # marker: [B, num_markers]\n",
    "        combined_gene = torch.cat([gene, marker], dim=1)  # [B, 512 + num_markers]\n",
    "\n",
    "        gene_emb = self.gene_encoder(combined_gene) + self.gene_type\n",
    "        morph_emb = self.morph_encoder(morph) + self.morph_type\n",
    "\n",
    "        spatial_emb = self.spatial_pe(spatial)  # [B, d_model]\n",
    "\n",
    "        tokens = torch.stack([gene_emb, morph_emb], dim=1)  # [B, 2, d_model]\n",
    "        spatial_emb = spatial_emb.unsqueeze(1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            tokens = layer(tokens, spatial_emb)\n",
    "\n",
    "        pooled = tokens.mean(dim=1)\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "    \n",
    "# Data Handling\n",
    "\n",
    "class CellDataset(Dataset):\n",
    "    def __init__(self, gene_df, morph_df, spatial_df, marker_df, labels):\n",
    "        # Convert everything to torch.Tensor\n",
    "        self.gene = torch.tensor(gene_df.values, dtype=torch.float32)\n",
    "        self.morph = torch.tensor(morph_df.values, dtype=torch.float32)\n",
    "        self.spatial = torch.tensor(spatial_df.values, dtype=torch.float32)\n",
    "        self.marker = torch.tensor(marker_df.values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gene_sample = self.gene[idx]       # [512]\n",
    "        morph_sample = self.morph[idx]     # [1536]\n",
    "        spatial_sample = self.spatial[idx] # [2]\n",
    "        marker_sample = self.marker[idx]   # [num_markers]\n",
    "        label = self.labels[idx]\n",
    "        return gene_sample, morph_sample, spatial_sample, marker_sample, label\n",
    "\n",
    "def compute_class_weights(labels):\n",
    "    counts = np.bincount(labels.values)\n",
    "    weights = 1. / (counts + 1e-8)  # Prevent division by zero\n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Training and testing loop\n",
    "\n",
    "num_epochs = 20 \n",
    "batch_size = 64\n",
    "\n",
    "# Initialize device and model\n",
    "device = torch.device(\"cuda\")\n",
    "num_markers = len(valid_markers)\n",
    "model = CellTransformer(\n",
    "    d_model=512,       # same as before\n",
    "    num_heads=8,\n",
    "    num_classes=num_classes, \n",
    "    num_markers=num_markers\n",
    ").to(device)\n",
    "    \n",
    "    \n",
    "# model = CellTransformer(d_model=512, num_heads=8, num_classes=num_classes).to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
    "# dataset = CellDataset(gene_embeddings, morph_embeddings, spatial_coords, labels)\n",
    "\n",
    "dataset = CellDataset(\n",
    "    gene_embeddings,       # shape [N, 512]\n",
    "    morph_embeddings,      # shape [N, 1536]\n",
    "    spatial_coords,        # shape [N, 2]\n",
    "    marker_expr_df,        # shape [N, num_markers]\n",
    "    labels                 # shape [N]\n",
    ")\n",
    "\n",
    "\n",
    "# Split dataset into 80% train and 20% test (stratified by labels)\n",
    "all_indices = np.arange(len(dataset))\n",
    "train_idx, test_idx = train_test_split(\n",
    "    all_indices, test_size=0.2, random_state=42, stratify=dataset.labels.numpy()\n",
    ")\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "test_dataset = Subset(dataset, test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=3e-5, steps_per_epoch=len(train_loader), epochs=num_epochs\n",
    ")\n",
    "\n",
    "if use_focal_loss:\n",
    "        criterion = FocalLoss(\n",
    "        gamma=2.0,  # typical default\n",
    "        alpha=compute_class_weights(labels).to(device),  # or a scalar, or None\n",
    "        reduction='mean'\n",
    "    ).to(device)\n",
    "\n",
    "else:\n",
    "    # Loss function with class weights moved to the proper device\n",
    "    criterion = nn.CrossEntropyLoss(weight=compute_class_weights(labels).to(device))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Wrap the train_loader with tqdm for a progress bar\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "    for batch_idx, (gene, morph, spatial, marker, lbls) in enumerate(progress_bar):\n",
    "        gene, morph, spatial, marker, lbls = (\n",
    "            gene.to(device),\n",
    "            morph.to(device),\n",
    "            spatial.to(device),\n",
    "            marker.to(device),\n",
    "            lbls.to(device)\n",
    "        )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(gene, morph, spatial, marker)\n",
    "        loss = criterion(outputs, lbls)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss += loss.item() * gene.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += lbls.size(0)\n",
    "        correct += (predicted == lbls).sum().item()\n",
    "\n",
    "        # Update progress bar with current loss\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = epoch_loss / total\n",
    "    accuracy = correct / total * 100 \n",
    "    print(f\"Epoch {epoch+1}: Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # Evaluation on the test set\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for gene, morph, spatial, marker, lbls in test_loader:\n",
    "            gene, morph, spatial, marker, lbls = (\n",
    "                gene.to(device),\n",
    "                morph.to(device),\n",
    "                spatial.to(device),\n",
    "                marker.to(device),\n",
    "                lbls.to(device)\n",
    "            )\n",
    "        \n",
    "            outputs = model(gene, morph, spatial, marker)\n",
    "            loss = criterion(outputs, lbls)\n",
    "            test_loss += loss.item() * gene.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_total += lbls.size(0)\n",
    "            test_correct += (predicted == lbls).sum().item()\n",
    "    avg_test_loss = test_loss / test_total\n",
    "    test_accuracy = test_correct / test_total * 100\n",
    "    print(f\"Test: Loss: {avg_test_loss:.4f} | Accuracy: {test_accuracy:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eec6382-9ad9-43e3-8e5e-5fe374925202",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e225f3-3e49-4179-a113-139012cdf1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval on same data \n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for gene, morph, spatial, marker, lbls in test_loader:\n",
    "        gene, morph, spatial, marker, lbls = (\n",
    "            gene.to(device),\n",
    "            morph.to(device),\n",
    "            spatial.to(device),\n",
    "            marker.to(device),\n",
    "            lbls.to(device)\n",
    "        )\n",
    "\n",
    "        outputs = model(gene, morph, spatial, marker)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(lbls.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=target_classes)\n",
    "\n",
    "\n",
    "disp.plot(cmap='viridis', xticks_rotation='vertical')\n",
    "plt.title(f\"Confusion Matrix: {platform}\")\n",
    "plt.show()\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=target_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334192c5-d868-4ee0-9330-20800f7052ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a92d96-2083-41ab-a3bc-ba2444a65225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%Eval trained model on different data \n",
    "# Select platform\n",
    "platform = \"xenium\" # xenium or visium \n",
    "ground_truth = \"refined\"  # refined or cellvit\n",
    "level = 0\n",
    "\n",
    "#%% Define Class Limiting Parameters\n",
    "limit_classes = True  # Set to False to use all classes\n",
    "target_classes = [\"f\", \"l\", \"t\"]  # Modify this list to restrict classification to specific classes\n",
    "\n",
    "if platform == \"xenium\":\n",
    "    cancer = \"breast\"\n",
    "    xenium_folder_dict = {\"lung\": \"Xenium_Prime_Human_Lung_Cancer_FFPE_outs\",\n",
    "                          \"breast\":\"Xenium_Prime_Breast_Cancer_FFPE_outs\",\n",
    "                          \"lymph_node\": \"Xenium_Prime_Human_Lymph_Node_Reactive_FFPE_outs\",\n",
    "                          \"prostate\": \"Xenium_Prime_Human_Prostate_FFPE_outs\",\n",
    "                          \"skin\": \"Xenium_Prime_Human_Skin_FFPE_outs\",\n",
    "                          \"ovarian\": \"Xenium_Prime_Ovarian_Cancer_FFPE_outs\",\n",
    "                          \"cervical\": \"Xenium_Prime_Cervical_Cancer_FFPE_outs\"\n",
    "                          }\n",
    "\n",
    "    xenium_folder = xenium_folder_dict[cancer]\n",
    "    \n",
    "    data_path = f\"/rsrch9/home/plm/idso_fa1_pathology/TIER1/paul-xenium/public_data/10x_genomics/{xenium_folder}/preprocessed/fine_tune_{ground_truth}_v2/processed_xenium_data_fine_tune_{ground_truth}_v2.h5ad\"\n",
    "    gene_embedding_file = f\"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/embeddings/public_data/{xenium_folder}/processed_xenium_{ground_truth}_v2.csv\"\n",
    "    \n",
    "    # Load Morphological Embeddings\n",
    "    morph_embedding_dir = f\"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/embeddings/public_data/{xenium_folder}\"\n",
    "\n",
    "        \n",
    "elif platform == \"visium\":\n",
    "    data_path = \"/rsrch9/home/plm/idso_fa1_pathology/TIER1/paul-xenium/public_data/10x_genomics/Visium_HD_Human_Lung_Cancer_post_Xenium_Prime_5k_Experiment2/binned_outputs/square_002um/preprocessed/bin2cell/to_tokenize/corrected_cells_matched_preprocessed_refined_v2.h5ad\"\n",
    "\n",
    "    gene_embedding_file = \"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/embeddings/public_data/Visium_HD_Human_Lung_Cancer_post_Xenium_Prime_5k_Experiment2/bin2cell/embeddings_output/processed_visium_hd_bin2cell.csv\"\n",
    "    morph_embedding_dir = f\"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/embeddings/public_data/Visium_HD_Human_Lung_Cancer_post_Xenium_Prime_5k_Experiment2\"\n",
    "\n",
    "# Load AnnData\n",
    "adata = sc.read_h5ad(data_path)\n",
    "cell_data = adata.obs\n",
    "\n",
    "# Spatial Information \n",
    "spatial_coords = cell_data[['x_centroid', 'y_centroid']].rename(columns={'x_centroid': 'x', 'y_centroid': 'y'})\n",
    "\n",
    "# Load gene Embeddings \n",
    "gene_embeddings = pd.read_csv(gene_embedding_file, index_col=\"Unnamed: 0\")\n",
    "gene_embeddings.index = cell_data.index\n",
    "\n",
    "# Load Morphology Embeddings \n",
    "morph_embedding_csv = os.path.join(morph_embedding_dir, \"UNI2_cell_representation\",f\"level_{level}\",\"morphology_embeddings_v2.csv\")\n",
    "morph_embeddings = pd.read_csv(morph_embedding_csv, index_col=\"Unnamed: 0\")\n",
    "\n",
    "\n",
    "if limit_classes:\n",
    "    num_classes = len(target_classes)\n",
    "    cell_data = cell_data[cell_data[\"class\"].isin(target_classes)]\n",
    "    \n",
    "    # Change index type for Visium data to match embeddings Idxs \n",
    "    if platform == \"visium\":\n",
    "        morph_embeddings.index = morph_embeddings.index.astype(str)\n",
    "\n",
    "    # Update corresponding embeddings and spatial coordinates\n",
    "    gene_embeddings = gene_embeddings.reindex(cell_data.index)\n",
    "    morph_embeddings = morph_embeddings.reindex(cell_data.index)\n",
    "    spatial_coords = spatial_coords.reindex(cell_data.index)\n",
    "else:\n",
    "    target_classes = [\"f\",\"l\",\"o\",\"t\"]\n",
    "    \n",
    "# Convert labels to numerical indices\n",
    "label_mapping = {l:c for c,l in enumerate(target_classes)}\n",
    "labels = pd.Series([label_mapping[lbl] for lbl in cell_data[\"class\"]])\n",
    "\n",
    "dataset = CellDataset(gene_embeddings, morph_embeddings, spatial_coords, labels)\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for gene, morph, spatial, lbls in test_loader:\n",
    "        gene = gene.to(device)\n",
    "        morph = morph.to(device)\n",
    "        spatial = spatial.to(device)\n",
    "        lbls = lbls.to(device)\n",
    "\n",
    "        outputs = model(gene, morph, spatial)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(lbls.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=target_classes)\n",
    "\n",
    "\n",
    "disp.plot(cmap='viridis', xticks_rotation='vertical')\n",
    "plt.title(f\"Confusion Matrix: {platform}\")\n",
    "plt.show()\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afa9877-7401-4412-b344-56ad36f7fc2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phacosta (py3.10.12)",
   "language": "python",
   "name": "phacosta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
