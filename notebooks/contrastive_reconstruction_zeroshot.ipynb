{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbb4ac7c-46aa-47e7-b7a5-2cd2622bb038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Contrastive + Reconstruction fine‑tuning\n",
    "=======================================\n",
    "\n",
    "*   UNI2 backbone (ViT) + projection heads (InfoNCE, as before)\n",
    "*   NEW: MLP regressor that tries to reconstruct the full Xenium 5 k\n",
    "    log‑normalised expression profile from the centre‑token embedding.\n",
    "*   Joint loss = InfoNCE + λ·MSE  (λ default = 0.1)\n",
    "*   Checkpoints saved every epoch + best checkpoint on lowest joint loss\n",
    "-----------------------------------------------------------------------\n",
    "\"\"\"\n",
    "# ---------------------------------------------------------------------\n",
    "# 0. Imports\n",
    "# ---------------------------------------------------------------------\n",
    "import os, torch, timm, scanpy as sc\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import pandas as pd, numpy as np, openslide\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1d0439c-817d-4e3b-866d-430aba5f77d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 1. User params\n",
    "# ---------------------------------------------------------------------\n",
    "cancer = \"lung\"          # {lung, breast, …}\n",
    "ground_truth = \"refined\"\n",
    "batch_size = 128\n",
    "num_workers = 4\n",
    "proj_dim = 128\n",
    "gene_dim = 5001            # Xenium Prime 5 k panel\n",
    "lr = 1e-4\n",
    "epochs = 10\n",
    "lambda_mse = 0.1             # weight on reconstruction loss\n",
    "level = 0               # centre‑token level (0 or 1)\n",
    "freeze_uni2 = True           # set True for baseline\n",
    "ckpt_dir = \"/rsrch5/home/plm/phacosta/models/fine_tuned/gene_reconstruction/ckpts_contrastive_recon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bceda4b-49e1-4a7c-9c2a-a9278d13b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 2. Paths\n",
    "# ---------------------------------------------------------------------\n",
    "xenium_sample_dict = {\n",
    "    \"lung\":       \"Xenium_Prime_Human_Lung_Cancer_FFPE_outs\",\n",
    "    \"breast\":     \"Xenium_Prime_Breast_Cancer_FFPE_outs\",\n",
    "    \"lymph_node\": \"Xenium_Prime_Human_Lymph_Node_Reactive_FFPE_outs\",\n",
    "    \"prostate\":   \"Xenium_Prime_Human_Prostate_FFPE_outs\",\n",
    "    \"skin\":       \"Xenium_Prime_Human_Skin_FFPE_outs\",\n",
    "    \"ovarian\":    \"Xenium_Prime_Ovarian_Cancer_FFPE_outs\",\n",
    "    \"cervical\":   \"Xenium_Prime_Cervical_Cancer_FFPE_outs\",\n",
    "}\n",
    "data_root   = \"/rsrch9/home/plm/idso_fa1_pathology/TIER1/paul-xenium/public_data/10x_genomics\"\n",
    "embedding_root = \"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/embeddings\"\n",
    "xenium_sample  = xenium_sample_dict[cancer]\n",
    "\n",
    "root = \"/rsrch9/home/plm/idso_fa1_pathology/TIER1/paul-xenium/public_data/10x_genomics\"\n",
    "adata_path = f\"{data_root}/{xenium_sample}/preprocessed/fine_tune_{ground_truth}_v2/processed_xenium_data_fine_tune_{ground_truth}_v2_annotated.h5ad\"\n",
    "emb_path = f\"{embedding_root}/public_data/{xenium_sample}/scGPT_CP.h5ad\"\n",
    "slide_path = f\"{data_root}/{xenium_sample}/{xenium_sample.rsplit('_',1)[0]}_he_image_registered.ome.tif\" \n",
    "\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Load data\n",
    "# ---------------------------------------------------------------------\n",
    "adata = sc.read_h5ad(adata_path)\n",
    "cell_df = adata.obs                       # index = cell IDs\n",
    "gdata = sc.read_h5ad(emb_path)\n",
    "gene_emb = pd.DataFrame(gdata.obsm[\"X_scGPT\"], index=cell_df.index)  # (N,512)\n",
    "\n",
    "slide = openslide.open_slide(slide_path)\n",
    "mpp_x = float(slide.properties.get(\"openslide.comment\").split('PhysicalSizeX=\"')[1].split('\"')[0])\n",
    "current_mpp= mpp_x\n",
    "target_mpp = 0.5           # 20×\n",
    "scale = target_mpp / current_mpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31fe7cd4-b7db-4d9f-acba-cfc571f933a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 4. Dataset & DataLoader\n",
    "# ---------------------------------------------------------------------\n",
    "patch_size = 224\n",
    "tfm = transforms.Compose([\n",
    "    transforms.Resize((patch_size, patch_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406),\n",
    "                         std =(0.229,0.224,0.225)),\n",
    "])\n",
    "\n",
    "class CellPatchDS(Dataset):\n",
    "    def __init__(self, slide, cells, tfm, scale, size):\n",
    "        self.slide, self.cells, self.tfm = slide, cells.reset_index(drop=False), tfm\n",
    "        self.scale, self.size = scale, size\n",
    "    def __len__(self): return len(self.cells)\n",
    "    def _patch(self, x,y):\n",
    "        big = int(self.size*self.scale)\n",
    "        tlx, tly = int(x-big/2), int(y-big/2)\n",
    "        img = self.slide.read_region((tlx,tly),0,(big,big)).convert(\"RGB\")\n",
    "        return img.resize((self.size,self.size), Image.LANCZOS)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.cells.iloc[i]\n",
    "        return self.tfm(self._patch(r.x_centroid,r.y_centroid)), i\n",
    "\n",
    "loader = DataLoader(\n",
    "    CellPatchDS(slide, cell_df, tfm, scale, patch_size),\n",
    "    batch_size, shuffle=True, num_workers=num_workers,\n",
    "    pin_memory=True, persistent_workers=True, prefetch_factor=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9039e3e9-df9e-4f76-844b-a6076a7b74f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  15%|█▌        | 288/1912 [05:11<29:14,  1.08s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m m_proj \u001b[38;5;241m=\u001b[39m proj_morph(center)                          \u001b[38;5;66;03m# (B,128)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m expr_pred \u001b[38;5;241m=\u001b[39m reg_head(center)                         \u001b[38;5;66;03m# (B,5000)\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m true_expr \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43madata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_np\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39m(info_nce(g_proj,m_proj)\u001b[38;5;241m+\u001b[39minfo_nce(m_proj,g_proj))\n\u001b[1;32m     81\u001b[0m mse  \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(expr_pred, true_expr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 5. Models\n",
    "# ---------------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "uni2_cfg = {\n",
    "    'model_name':'vit_giant_patch14_224','img_size':224,'patch_size':14,'depth':24,\n",
    "    'num_heads':24,'init_values':1e-5,'embed_dim':1536,'mlp_ratio':2.66667*2,\n",
    "    'num_classes':0,'no_embed_class':True,'mlp_layer':timm.layers.SwiGLUPacked,\n",
    "    'act_layer':torch.nn.SiLU,'reg_tokens':8,'dynamic_img_size':True\n",
    "}\n",
    "model = timm.create_model(pretrained=False, **uni2_cfg)\n",
    "model.load_state_dict(torch.load(\"/rsrch5/home/plm/phacosta/models/public/UNI2-h/pytorch_model.bin\", map_location=\"cpu\"))\n",
    "model.to(device).train()\n",
    "if freeze_uni2:\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "prefix_tokens = getattr(model, \"num_prefix_tokens\", 9)\n",
    "\n",
    "level_idx = {\n",
    "    0: torch.tensor([119,120,135,136], device=device),\n",
    "    1: torch.tensor([102,103,104,105,118,119,120,121,\n",
    "                     134,135,136,137,150,151,152,153], device=device)\n",
    "}[level]\n",
    "\n",
    "class Projection(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(nn.Linear(in_dim,256), nn.ReLU(), nn.Linear(256,out_dim))\n",
    "    def forward(self,x): return self.mlp(x)\n",
    "\n",
    "proj_gene  = Projection(gene_emb.shape[1], proj_dim).to(device)\n",
    "proj_morph = Projection(1536, proj_dim).to(device)\n",
    "\n",
    "reg_head = nn.Sequential(\n",
    "    nn.Linear(1536,512), nn.ReLU(),\n",
    "    nn.Linear(512,gene_dim)\n",
    ").to(device)\n",
    "\n",
    "def info_nce(a,p,t=0.07):\n",
    "    a,p = F.normalize(a,dim=1), F.normalize(p,dim=1)\n",
    "    return F.cross_entropy(a @ p.T / t, torch.arange(a.size(0), device=a.device))\n",
    "\n",
    "opt = optim.Adam(\n",
    "    list(filter(lambda p: p.requires_grad, model.parameters())) +\n",
    "    list(proj_gene.parameters()) +\n",
    "    list(proj_morph.parameters()) +\n",
    "    list(reg_head.parameters()),\n",
    "    lr=lr\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6. Training\n",
    "# ---------------------------------------------------------------------\n",
    "best_loss = float(\"inf\")\n",
    "for ep in range(1, epochs+1):\n",
    "    run_info, run_mse = 0.0, 0.0\n",
    "    model.train(); proj_gene.train(); proj_morph.train(); reg_head.train()\n",
    "    for imgs, idx in tqdm(loader, desc=f\"Epoch {ep}\"):\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        idx_np = idx.numpy()\n",
    "        gene_batch = torch.as_tensor(\n",
    "            gene_emb.iloc[idx_np].values, dtype=torch.float32\n",
    "        ).to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Forward\n",
    "        tok   = model.forward_features(imgs)                 # (B,265,1536)\n",
    "        spat  = tok[:, prefix_tokens:, :]\n",
    "        center= spat[:, level_idx, :].mean(1)                # (B,1536)\n",
    "\n",
    "        g_proj = proj_gene(gene_batch)                       # (B,128)\n",
    "        m_proj = proj_morph(center)                          # (B,128)\n",
    "        expr_pred = reg_head(center)                         # (B,5000)\n",
    "        true_expr = torch.as_tensor(\n",
    "            adata.X[idx_np].toarray(), dtype=torch.float32\n",
    "        ).to(device, non_blocking=True)\n",
    "\n",
    "        info = 0.5*(info_nce(g_proj,m_proj)+info_nce(m_proj,g_proj))\n",
    "        mse  = F.mse_loss(expr_pred, true_expr)\n",
    "        loss = info + lambda_mse*mse\n",
    "\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        run_info += info.item(); run_mse += mse.item()\n",
    "\n",
    "    info_avg = run_info/len(loader); mse_avg = run_mse/len(loader)\n",
    "    print(f\"Epoch {ep}/{epochs} | InfoNCE={info_avg:.4f} | MSE={mse_avg:.4f}\")\n",
    "\n",
    "    ckpt = {\n",
    "        \"epoch\": ep, \"model\": model.state_dict(),\n",
    "        \"proj_gene\": proj_gene.state_dict(),\n",
    "        \"proj_morph\": proj_morph.state_dict(),\n",
    "        \"reg_head\": reg_head.state_dict(),\n",
    "        \"opt\": opt.state_dict(),\n",
    "        \"loss\": float(loss)\n",
    "    }\n",
    "    torch.save(ckpt, f\"{ckpt_dir}/epoch_{ep:03d}.pth\")\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        torch.save(ckpt, f\"{ckpt_dir}/best.pth\")\n",
    "        print(\"✓ new best checkpoint\")\n",
    "\n",
    "print(\"Training done.  Best joint loss:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9512c68e-5a72-45df-afe3-4a2a8db5711c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phacosta (py3.10.12)",
   "language": "python",
   "name": "phacosta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
