{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30a1f2-6773-4e38-83e2-4a4abe160a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "End‑to‑end contrastive fine‑tuning of UNI2 with on‑the‑fly patch extraction\n",
    "and DataLoader prefetching.\n",
    "\n",
    "* Extracts cell‑centred patches directly from the WSI with OpenSlide.\n",
    "* Uses a torch Dataset/DataLoader (multi‑worker, pinned memory) so JPEG\n",
    "  decoding overlaps GPU compute.\n",
    "* Drops CLS + REG tokens, selects centre spatial tokens (level 1 by default).\n",
    "* Fine‑tunes UNI2 jointly with projection heads via InfoNCE loss against\n",
    "  fixed scGPT gene embeddings.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openslide\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fa6d6f-d3ad-4445-9d08-d6cf5d6099ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Define parameters\n",
    "# ---------------------------------------------------------------------\n",
    "cancer = \"lung\"          # {lung, breast, …}\n",
    "ground_truth = \"refined\"      # dataset variant\n",
    "level = 0               # centre‑token level (0 or 1 here)\n",
    "batch_size = 72\n",
    "num_workers = 8               # DataLoader workers (tune to CPU cores)\n",
    "proj_dim = 128             # dimension of joint embedding space\n",
    "lr = 1e-5\n",
    "epochs = 10\n",
    "\n",
    "#  ---- paths ---------------------------------------------------------\n",
    "xenium_sample_dict = {\n",
    "    \"lung\":       \"Xenium_Prime_Human_Lung_Cancer_FFPE_outs\",\n",
    "    \"breast\":     \"Xenium_Prime_Breast_Cancer_FFPE_outs\",\n",
    "    \"lymph_node\": \"Xenium_Prime_Human_Lymph_Node_Reactive_FFPE_outs\",\n",
    "    \"prostate\":   \"Xenium_Prime_Human_Prostate_FFPE_outs\",\n",
    "    \"skin\":       \"Xenium_Prime_Human_Skin_FFPE_outs\",\n",
    "    \"ovarian\":    \"Xenium_Prime_Ovarian_Cancer_FFPE_outs\",\n",
    "    \"cervical\":   \"Xenium_Prime_Cervical_Cancer_FFPE_outs\",\n",
    "}\n",
    "root   = \"/rsrch9/home/plm/idso_fa1_pathology/TIER1/paul-xenium/public_data/10x_genomics\"\n",
    "xenium_sample  = xenium_sample_dict[cancer]\n",
    "adata_path = f\"{root}/{xenium_sample}/preprocessed/fine_tune_{ground_truth}_v2/processed_xenium_data_fine_tune_{ground_truth}_v2_annotated.h5ad\"\n",
    "emb_path  = f\"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/embeddings/public_data/{xenium_sample}/scGPT_CP.h5ad\"\n",
    "slide_path= f\"{root}/{xenium_sample}/Xenium_Prime_Human_Lung_Cancer_FFPE_he_image_coregistered_pyramid.ome.tif\"  # adjust for cancer type if needed\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Load cell metadata & gene embeddings (fixed)\n",
    "# ---------------------------------------------------------------------\n",
    "adata = sc.read_h5ad(adata_path)\n",
    "cell_df = adata.obs                     # index = cell IDs\n",
    "gdata = sc.read_h5ad(emb_path)\n",
    "gene_emb = pd.DataFrame(gdata.obsm[\"X_scGPT\"], index=cell_df.index)\n",
    "\n",
    "print(\"Cells:\", cell_df.shape[0])\n",
    "print(\"Gene‑embedding dim:\", gene_emb.shape[1])\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Slide info (MPP)\n",
    "# ---------------------------------------------------------------------\n",
    "slide = openslide.open_slide(slide_path)\n",
    "mpp_x = float(slide.properties.get(\"openslide.comment\").split('PhysicalSizeX=\"')[1].split('\"')[0])\n",
    "current_mpp= mpp_x\n",
    "print(\"Slide MPP:\", current_mpp)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Torch vision transform for UNI2\n",
    "# ---------------------------------------------------------------------\n",
    "patch_size = 224\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((patch_size, patch_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "])\n",
    "\n",
    "target_mpp = 0.5  # 20×\n",
    "scale = target_mpp / current_mpp\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Dataset with on‑the‑fly patch extraction\n",
    "# ---------------------------------------------------------------------\n",
    "class CellPatchDataset(Dataset):\n",
    "    def __init__(self, slide, cell_df, transform, scale, patch_size):\n",
    "        self.slide      = slide\n",
    "        self.cells      = cell_df.reset_index(drop=False)  # keep cell IDs\n",
    "        self.tfm        = transform\n",
    "        self.scale      = scale\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cells)\n",
    "\n",
    "    def _read_patch(self, x, y):\n",
    "        big = int(self.patch_size * self.scale)\n",
    "        tlx, tly = int(x - big/2), int(y - big/2)\n",
    "        patch = self.slide.read_region((tlx, tly), 0, (big, big)).convert(\"RGB\")\n",
    "        return patch.resize((self.patch_size, self.patch_size), Image.LANCZOS)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.cells.iloc[idx]\n",
    "        patch = self._read_patch(row.x_centroid, row.y_centroid)\n",
    "        img_t = self.tfm(patch)\n",
    "        return img_t, idx\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# DataLoader with prefetching\n",
    "# ---------------------------------------------------------------------\n",
    "dataloader = DataLoader(\n",
    "    CellPatchDataset(slide, cell_df, transform, scale, patch_size),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ada2e-4127-4da8-a12f-c51b54a95bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# UNI2 model (trainable)\n",
    "# ---------------------------------------------------------------------\n",
    "model_dir = \"/rsrch5/home/plm/phacosta/models/public/UNI2-h\"\n",
    "uni2_cfg = {\n",
    "    'model_name':'vit_giant_patch14_224','img_size':224,'patch_size':14,'depth':24,\n",
    "    'num_heads':24,'init_values':1e-5,'embed_dim':1536,'mlp_ratio':2.66667*2,\n",
    "    'num_classes':0,'no_embed_class':True,'mlp_layer':timm.layers.SwiGLUPacked,\n",
    "    'act_layer':torch.nn.SiLU,'reg_tokens':8,'dynamic_img_size':True\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = timm.create_model(pretrained=False, **uni2_cfg)\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, \"pytorch_model.bin\"), map_location=\"cpu\"), strict=True)\n",
    "model.to(device).train()\n",
    "prefix_tokens = getattr(model, \"num_prefix_tokens\", 9)  # fallback reg_tokens + cls \n",
    "\n",
    "level_idx_map = {\n",
    "    0: torch.tensor([119,120,135,136]),\n",
    "    1: torch.tensor([102,103,104,105,118,119,120,121,134,135,136,137,150,151,152,153]),\n",
    "}\n",
    "idx_center = level_idx_map[level].to(device)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Projection heads & loss\n",
    "# ---------------------------------------------------------------------\n",
    "class Projection(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(nn.Linear(in_dim,256), nn.ReLU(), nn.Linear(256,out_dim))\n",
    "    def forward(self,x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "proj_gene = Projection(gene_emb.shape[1], proj_dim).to(device)\n",
    "proj_morph = Projection(1536, proj_dim).to(device)\n",
    "\n",
    "def info_nce(a, p, t=0.07):\n",
    "    a, p = F.normalize(a, dim=1), F.normalize(p, dim=1)\n",
    "    return F.cross_entropy(a @ p.T / t, torch.arange(a.size(0), device=a.device))\n",
    "\n",
    "opt = optim.Adam(list(model.parameters()) + list(proj_gene.parameters()) + list(proj_morph.parameters()), lr=lr)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Checkpoint directory & training loop\n",
    "# ---------------------------------------------------------------------\n",
    "ckpt_dir = \"/rsrch5/home/plm/phacosta/models/fine_tuned/UNI2/finetuned_uni2_contrastive\"\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running = 0.0\n",
    "    for imgs, idx_batch in tqdm(dataloader, desc=f\"Epoch {epoch}\"):\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        idx_np = idx_batch.cpu().numpy()\n",
    "        gene_batch = torch.as_tensor(\n",
    "            gene_emb.iloc[idx_np].values, dtype=torch.float32,\n",
    "        ).to(device, non_blocking=True)\n",
    "\n",
    "        tokens  = model.forward_features(imgs)\n",
    "        spatial = tokens[:, prefix_tokens:, :]\n",
    "        center  = spatial[:, idx_center, :].mean(1)\n",
    "\n",
    "        g_proj = proj_gene(gene_batch)\n",
    "        m_proj = proj_morph(center)\n",
    "        loss   = 0.5 * (info_nce(g_proj, m_proj) + info_nce(m_proj, g_proj))\n",
    "\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        running += loss.item()\n",
    "\n",
    "    avg_loss = running / len(dataloader)\n",
    "    print(f\"Epoch {epoch}/{epochs} | Avg loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # ---- checkpoint every epoch ----\n",
    "    ckpt = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model\": model.state_dict(),\n",
    "        \"proj_gene\": proj_gene.state_dict(),\n",
    "        \"proj_morph\": proj_morph.state_dict(),\n",
    "        \"optimizer\": opt.state_dict(),\n",
    "        \"avg_loss\": avg_loss,\n",
    "    }\n",
    "    torch.save(ckpt, f\"{ckpt_dir}/epoch_{epoch:03d}.pth\")\n",
    "\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        torch.save(ckpt, f\"{ckpt_dir}/best.pth\")\n",
    "        print(\"Saved new best checkpoint\")\n",
    "\n",
    "print(\"✓ Training complete. Best loss:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f8b750-273e-4d4e-95b6-76cb3536831f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phacosta (py3.10.12)",
   "language": "python",
   "name": "phacosta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
