{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c11f1d89-8965-4300-8e2b-6429a0101936",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BLEEP‑style contrastive training with UNI2 + scGPT\n",
    "--------------------------------------------------\n",
    "This script mirrors the original BLEEP training loop (DDP‑ready, `ProjectionHead`,\n",
    "smoothed CLIP loss) but swaps in:\n",
    "• **UNI2** as the image encoder (fine‑tuned)\n",
    "• **Pre‑extracted scGPT gene embeddings** as the gene branch (optionally frozen)\n",
    "• **Cell‑level patches** instead of Visium spots\n",
    "\n",
    "Key difference from the first draft: **the explicit `F.normalize` calls on the\n",
    "embeddings have been removed** to exactly match the original BLEEP loss.\n",
    "\"\"\"\n",
    "\n",
    "import os, torch, timm, openslide, scanpy as sc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bf4fa5e-c565-4956-b5a9-e5e1d5043516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Configuration (matching origianl BLEEP. Edit as needed)\n",
    "# -----------------------------------------------------------------------------\n",
    "class CFG:\n",
    "    # data\n",
    "    cancer = \"lung\"          # {lung, breast, …}\n",
    "    ground_truth = \"refined\"       # dataset variant\n",
    "    level = 0              # UNI2 spatial‑token level\n",
    "    batch_size = 72\n",
    "    num_workers = 8\n",
    "\n",
    "    # optimisation\n",
    "    temperature = 1.0\n",
    "    patience = 2.0\n",
    "    projection_dim= 256\n",
    "    lr = 1e-4\n",
    "    weight_decay = 1e-3\n",
    "    dropout = 0.1\n",
    "    epochs = 10\n",
    "\n",
    "    # Embeddings\n",
    "    morph_emb_dims = 1536\n",
    "    gene_emb_dims = 512\n",
    "    patch_size = 224\n",
    "\n",
    "    \n",
    "    # paths\n",
    "    root = \"/rsrch9/home/plm/idso_fa1_pathology/TIER1/paul-xenium/public_data/10x_genomics\"\n",
    "    xenium_sample_dict = {\n",
    "        \"lung\":\"Xenium_Prime_Human_Lung_Cancer_FFPE_outs\",\n",
    "    }\n",
    "    model_dir = \"/rsrch5/home/plm/phacosta/models/public/UNI2-h\"  # pretrained UNI2 weights\n",
    "    ckpt_dir = \"/rsrch5/home/plm/phacosta/models/fine_tuned/UNI2/bleep_style\"  # outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9781ba11-b485-4119-b547-6a2bdedde2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cells: 244659 | Gene‑embedding dim: 512\n",
      "Slide MPP: 0.2125\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Resolve dataset‑specific paths\n",
    "# -----------------------------------------------------------------------------\n",
    "sample  = CFG.xenium_sample_dict[CFG.cancer]\n",
    "adata_path = f\"{CFG.root}/{sample}/preprocessed/fine_tune_{CFG.ground_truth}_v2/processed_xenium_data_fine_tune_{CFG.ground_truth}_v2_annotated.h5ad\"\n",
    "emb_path  = f\"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/embeddings/public_data/{sample}/scGPT/scGPT_CP.h5ad\"\n",
    "slide_path= f\"{CFG.root}/{sample}/Xenium_Prime_Human_Lung_Cancer_FFPE_he_image_registered.ome.tif\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Load metadata & gene embeddings (fixed / optionally frozen)\n",
    "# -----------------------------------------------------------------------------\n",
    "adata   = sc.read_h5ad(adata_path)\n",
    "cell_df = adata.obs  # index = cell IDs (expects x_centroid / y_centroid cols)\n",
    "\n",
    "gdata   = sc.read_h5ad(emb_path)\n",
    "gene_emb = pd.DataFrame(gdata.obsm[\"X_scGPT\"], index=cell_df.index)\n",
    "print(\"Cells:\", cell_df.shape[0], \"| Gene‑embedding dim:\", gene_emb.shape[1])\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OpenSlide & image resolution\n",
    "# -----------------------------------------------------------------------------\n",
    "slide = openslide.open_slide(slide_path)\n",
    "MPP = float(slide.properties.get(\"openslide.comment\").split('PhysicalSizeX=\"')[1].split('\"')[0])\n",
    "print(\"Slide MPP:\", MPP)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Torch transformation & patch parameters\n",
    "# -----------------------------------------------------------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((CFG.patch_size, CFG.patch_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "])\n",
    "scale_factor = 0.5 / MPP  # rescale to ~20× (0.5µm/px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1a840da-8997-4104-9b9d-1a0987093115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 3399/3399 [1:42:18<00:00,  1.81s/batch, loss=3.06, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: avg loss 3.0613  |  time 102.3 min\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 3399/3399 [1:42:14<00:00,  1.80s/batch, loss=2.28, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: avg loss 2.2818  |  time 102.2 min\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 3399/3399 [1:42:08<00:00,  1.80s/batch, loss=2.23, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: avg loss 2.2288  |  time 102.1 min\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 3399/3399 [1:42:06<00:00,  1.80s/batch, loss=2.21, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: avg loss 2.2052  |  time 102.1 min\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 3399/3399 [1:42:03<00:00,  1.80s/batch, loss=2.2, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: avg loss 2.1985  |  time 102.1 min\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 3399/3399 [1:41:43<00:00,  1.80s/batch, loss=3.82, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: avg loss 3.8168  |  time 101.7 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 3399/3399 [1:41:49<00:00,  1.80s/batch, loss=2.2, lr=0.001] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: avg loss 2.1997  |  time 101.8 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 3399/3399 [1:41:44<00:00,  1.80s/batch, loss=2.18, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: avg loss 2.1846  |  time 101.7 min\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 3399/3399 [1:41:46<00:00,  1.80s/batch, loss=2.18, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: avg loss 2.1772  |  time 101.8 min\n",
      "✓ new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 3399/3399 [1:41:31<00:00,  1.79s/batch, loss=2.17, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: avg loss 2.1727  |  time 101.5 min\n",
      "✓ new best\n",
      "Training complete. Best loss: 2.1726769714995178\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Dataset: on‑the‑fly cell‑centred patch extraction\n",
    "# -----------------------------------------------------------------------------\n",
    "class CellPatchDataset(Dataset):\n",
    "    def __init__(self, slide, cell_df, gene_df, transform, scale, patch_size):\n",
    "        self.slide = slide\n",
    "        self.cells = cell_df.reset_index(drop=False)   # keeps cell IDs in col “index”\n",
    "        self.gene_df = gene_df                          # gene_emb DataFrame\n",
    "        self.tfm = transform\n",
    "        self.scale = scale\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cells)\n",
    "\n",
    "    def _read_patch(self, x, y):\n",
    "        big = int(self.patch_size * self.scale)\n",
    "        tlx, tly = int(x - big/2), int(y - big/2)\n",
    "        patch = self.slide.read_region((tlx, tly), 0, (big, big)).convert(\"RGB\")\n",
    "        return patch.resize((self.patch_size, self.patch_size), Image.LANCZOS)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row   = self.cells.iloc[idx]\n",
    "        patch = self._read_patch(row.x_centroid, row.y_centroid)\n",
    "        img_t = self.tfm(patch)\n",
    "\n",
    "        cell_id   = row[\"index\"]                              # original cell ID\n",
    "        gene_vec  = torch.tensor(\n",
    "            self.gene_df.loc[cell_id].values, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        return {\"image\": img_t, \"gene\": gene_vec}\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Projection head (identical to original BLEEP)\n",
    "# -----------------------------------------------------------------------------\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim, projection_dim=CFG.projection_dim, dropout=CFG.dropout):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc   = nn.Linear(projection_dim, projection_dim)\n",
    "        self.do   = nn.Dropout(dropout)\n",
    "        self.ln   = nn.LayerNorm(projection_dim)\n",
    "    def forward(self, x):\n",
    "        h = self.proj(x)\n",
    "        x = self.gelu(h)\n",
    "        x = self.fc(x)\n",
    "        x = self.do(x)\n",
    "        return self.ln(x + h)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# UNI2 image encoder (timm) – put into train mode for fine‑tuning\n",
    "# -----------------------------------------------------------------------------\n",
    "uni2_cfg = {\n",
    "    'model_name':'vit_giant_patch14_224','img_size':224,'patch_size':14,'depth':24,\n",
    "    'num_heads':24,'init_values':1e-5,'embed_dim':1536,'mlp_ratio':2.66667*2,\n",
    "    'num_classes':0,'no_embed_class':True,'mlp_layer':timm.layers.SwiGLUPacked,\n",
    "    'act_layer':torch.nn.SiLU,'reg_tokens':8,'dynamic_img_size':True,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "uni2 = timm.create_model(pretrained=False, **uni2_cfg)\n",
    "uni2.load_state_dict(torch.load(os.path.join(CFG.model_dir, \"pytorch_model.bin\"), map_location=\"cpu\"), strict=True)\n",
    "uni2 = uni2.to(device).train()\n",
    "\n",
    "prefix_tokens = getattr(uni2, \"num_prefix_tokens\", 9)\n",
    "level_idx_map = {\n",
    "    0: torch.tensor([119,120,135,136]),\n",
    "    1: torch.tensor([102,103,104,105,118,119,120,121,134,135,136,137,150,151,152,153]),\n",
    "}\n",
    "center_idx = level_idx_map[CFG.level].to(device)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Full BLEEP‑style model\n",
    "# -----------------------------------------------------------------------------\n",
    "class BLEEP_UNI2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_enc: nn.Module,\n",
    "        gene_dim: int,\n",
    "        morph_dim: int = CFG.morph_emb_dims,     # 1536\n",
    "        init_temp: float = CFG.temperature       # 1.0 → logit_scale = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # encoders + projection heads\n",
    "        self.image_encoder = img_enc\n",
    "        self.image_proj    = ProjectionHead(morph_dim)\n",
    "        self.gene_proj     = ProjectionHead(gene_dim)\n",
    "\n",
    "        # learnable logit-scale   (log(1/τ))\n",
    "        self.logit_scale = nn.Parameter(torch.log(torch.tensor(1.0 / init_temp)))\n",
    "\n",
    "        # register centre-token indices so they move with .to(device)\n",
    "        self.prefix_tokens = 9                           # CLS + 8 REG\n",
    "        self.register_buffer(\n",
    "            \"center_idx\",\n",
    "            torch.tensor([119, 120, 135, 136], dtype=torch.long),\n",
    "            persistent=False\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def _encode_image(self, imgs: torch.Tensor) -> torch.Tensor:\n",
    "        tok     = self.image_encoder.forward_features(imgs)        # (B, 265, 1536)\n",
    "        spatial = tok[:, self.prefix_tokens:, :]                  # drop prefixes\n",
    "        center  = spatial.index_select(1, self.center_idx).mean(1)\n",
    "        return self.image_proj(center)                            # (B,256)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def forward(self, imgs: torch.Tensor, genes: torch.Tensor) -> torch.Tensor:\n",
    "        #  embeddings (L2-normalised)\n",
    "        img_vec  = F.normalize(self._encode_image(imgs),  dim=-1)  # (B,256)\n",
    "        gene_vec = F.normalize(self.gene_proj(genes),     dim=-1)  # (B,256)\n",
    "\n",
    "        # contrastive logits with learnable temperature\n",
    "        scale  = self.logit_scale.exp()                            # scalar > 0\n",
    "        logits = scale * (gene_vec @ img_vec.T)                    # (B,B)\n",
    "\n",
    "        # smoothed intra-modal targets (no gradients needed)\n",
    "        with torch.no_grad():\n",
    "            sim_img  = scale * (img_vec  @ img_vec.T)\n",
    "            sim_gene = scale * (gene_vec @ gene_vec.T)\n",
    "            targets  = F.softmax(0.5 * (sim_img + sim_gene), dim=-1)  # (B,B)\n",
    "\n",
    "        # cross-entropy, symmetrised\n",
    "        loss_gene = F.cross_entropy(logits,   targets,   reduction='none')\n",
    "        loss_img  = F.cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        return 0.5 * (loss_gene + loss_img).mean()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# DataLoader\n",
    "# -----------------------------------------------------------------------------\n",
    "loader = DataLoader(\n",
    "    CellPatchDataset(slide, cell_df, gene_emb, transform, scale_factor, CFG.patch_size),\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=CFG.num_workers,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4,\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Optimiser & training loop\n",
    "# -----------------------------------------------------------------------------\n",
    "model = BLEEP_UNI2(uni2, gene_emb.shape[1]).to(device)\n",
    "opt   = torch.optim.AdamW(model.parameters(), lr=CFG.lr)\n",
    "\n",
    "os.makedirs(CFG.ckpt_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "best = float(\"inf\")\n",
    "for epoch in range(1, CFG.epochs + 1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    start   = time.time()\n",
    "\n",
    "    # wrap loader in tqdm → shows progress & ETA\n",
    "    prog = tqdm(loader, desc=f\"Epoch {epoch}/{CFG.epochs}\", unit=\"batch\")\n",
    "    for step, batch in enumerate(prog, 1):\n",
    "        imgs  = batch[\"image\"].to(device, non_blocking=True)\n",
    "        genes = batch[\"gene\"].to(device,  non_blocking=True)\n",
    "\n",
    "        loss = model(imgs, genes)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running += loss.item()\n",
    "        prog.set_postfix(loss=running / step, lr=opt.param_groups[0][\"lr\"])\n",
    "\n",
    "    avg = running / len(loader)\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Epoch {epoch}: avg loss {avg:.4f}  |  time {elapsed/60:.1f} min\")\n",
    "\n",
    "    torch.save(\n",
    "        {\"epoch\": epoch,\n",
    "         \"model\": model.state_dict(),\n",
    "         \"opt\":   opt.state_dict(),\n",
    "         \"loss\":  avg},\n",
    "        f\"{CFG.ckpt_dir}/epoch_{epoch:03d}.pth\"\n",
    "    )\n",
    "\n",
    "    if avg < best:\n",
    "        best = avg\n",
    "        torch.save({\"epoch\": epoch, \"model\": model.state_dict()},\n",
    "                   f\"{CFG.ckpt_dir}/best.pth\")\n",
    "        print(\"✓ new best\")\n",
    "\n",
    "print(\"Training complete. Best loss:\", best)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phacosta (py3.10.12)",
   "language": "python",
   "name": "phacosta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
