{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c11f1d89-8965-4300-8e2b-6429a0101936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BLEEP‑style contrastive training with UNI2 + scGPT\n",
    "--------------------------------------------------\n",
    "This script mirrors the original BLEEP training loop (DDP‑ready, `ProjectionHead`,\n",
    "smoothed CLIP loss) but swaps in:\n",
    "• **UNI2** as the image encoder (fine‑tuned)\n",
    "• **Pre‑extracted scGPT gene embeddings** as the gene branch (optionally frozen)\n",
    "• **Cell‑level patches** instead of Visium spots\n",
    "\n",
    "Key difference from the first draft: **the explicit `F.normalize` calls on the\n",
    "embeddings have been removed** to exactly match the original BLEEP loss.\n",
    "\"\"\"\n",
    "\n",
    "import os, torch, timm, openslide, scanpy as sc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True           # ← optional speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bf4fa5e-c565-4956-b5a9-e5e1d5043516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Configuration (matching origianl BLEEP. Edit as needed)\n",
    "# -----------------------------------------------------------------------------\n",
    "class CFG:\n",
    "    # data\n",
    "    cancer = \"lung\"          # {lung, breast, …}\n",
    "    ground_truth = \"refined\"       # dataset variant\n",
    "    level = 0              # UNI2 spatial‑token level\n",
    "    batch_size = 72\n",
    "    num_workers = 8\n",
    "\n",
    "    # optimisation\n",
    "    temperature = 1.0\n",
    "    patience = 2.0\n",
    "    projection_dim= 256\n",
    "    lr = 1e-4\n",
    "    weight_decay = 1e-3\n",
    "    dropout = 0.1\n",
    "    epochs = 10\n",
    "\n",
    "    # Embeddings\n",
    "    morph_emb_dims = 1536\n",
    "    gene_emb_dims = 512\n",
    "    patch_size = 224\n",
    "\n",
    "    # Regularization\n",
    "    use_L1_reg = False\n",
    "    l1_lambda = 1e-4\n",
    "    \n",
    "    # paths\n",
    "    root = \"/rsrch9/home/plm/idso_fa1_pathology/TIER1/paul-xenium/public_data/10x_genomics\"\n",
    "    xenium_sample_dict = {\n",
    "        \"lung\":\"Xenium_Prime_Human_Lung_Cancer_FFPE_outs\",\n",
    "    }\n",
    "    model_dir = \"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/models/public/UNI2-h\"  # pretrained UNI2 weights\n",
    "    ckpt_dir = \"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/models/fine_tuned/UNI2/bleep_style_testing\"  # outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20ad348c-85e4-4b14-b085-429700db64c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cells: 244659 | Gene‑embedding dim: 512\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Resolve dataset‑specific paths\n",
    "# -----------------------------------------------------------------------------\n",
    "sample  = CFG.xenium_sample_dict[CFG.cancer]\n",
    "adata_path = f\"{CFG.root}/{sample}/preprocessed/fine_tune_{CFG.ground_truth}_v2/processed_xenium_data_fine_tune_{CFG.ground_truth}_v2_annotated.h5ad\"\n",
    "emb_path  = f\"/rsrch9/home/plm/idso_fa1_pathology/TIER2/paul-xenium/embeddings/public_data/{sample}/scGPT/scGPT_CP.h5ad\"\n",
    "slide_path= f\"{CFG.root}/{sample}/Xenium_Prime_Human_Lung_Cancer_FFPE_he_image_registered.ome.tif\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Load metadata & gene embeddings (fixed / optionally frozen)\n",
    "# -----------------------------------------------------------------------------\n",
    "adata   = sc.read_h5ad(adata_path)\n",
    "cell_df = adata.obs  # index = cell IDs (expects x_centroid / y_centroid cols)\n",
    "\n",
    "gdata   = sc.read_h5ad(emb_path)\n",
    "gene_emb = pd.DataFrame(gdata.obsm[\"X_scGPT\"], index=cell_df.index)\n",
    "print(\"Cells:\", cell_df.shape[0], \"| Gene‑embedding dim:\", gene_emb.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dd7a7b2-da23-4a83-b18e-da118729f99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slide MPP: 0.2125\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# OpenSlide & image resolution\n",
    "# -----------------------------------------------------------------------------\n",
    "slide = openslide.open_slide(slide_path)\n",
    "MPP = float(slide.properties.get(\"openslide.comment\").split('PhysicalSizeX=\"')[1].split('\"')[0])\n",
    "print(\"Slide MPP:\", MPP)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Torch transformation & patch parameters\n",
    "# -----------------------------------------------------------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((CFG.patch_size, CFG.patch_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "])\n",
    "scale_factor = 0.5 / MPP  # rescale to ~20× (0.5µm/px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a840da-8997-4104-9b9d-1a0987093115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E1/10:   3%|▎         | 96/3399 [01:22<35:47,  1.54batch/s, loss=4.28, lr=0.001]  "
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Dataset: on‑the‑fly cell‑centred patch extraction\n",
    "# -----------------------------------------------------------------------------\n",
    "class CellPatchDataset(Dataset):\n",
    "    def __init__(self, slide, cell_df, gene_df, transform, scale, patch_size):\n",
    "        self.slide = slide\n",
    "        self.cells = cell_df.reset_index(drop=False)   # keeps cell IDs in col “index”\n",
    "        self.gene_df = gene_df                          # gene_emb DataFrame\n",
    "        self.tfm = transform\n",
    "        self.scale = scale\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cells)\n",
    "\n",
    "    def _read_patch(self, x, y):\n",
    "        big = int(self.patch_size * self.scale)\n",
    "        tlx, tly = int(x - big/2), int(y - big/2)\n",
    "        patch = self.slide.read_region((tlx, tly), 0, (big, big)).convert(\"RGB\")\n",
    "        return patch.resize((self.patch_size, self.patch_size), Image.LANCZOS)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row   = self.cells.iloc[idx]\n",
    "        patch = self._read_patch(row.x_centroid, row.y_centroid)\n",
    "        img_t = self.tfm(patch)\n",
    "\n",
    "        cell_id   = row[\"index\"]                              # original cell ID\n",
    "        gene_vec  = torch.tensor(\n",
    "            self.gene_df.loc[cell_id].values, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        return {\"image\": img_t, \"gene\": gene_vec}\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Projection head (identical to original BLEEP)\n",
    "# -----------------------------------------------------------------------------\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim, projection_dim=CFG.projection_dim, dropout=CFG.dropout):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc   = nn.Linear(projection_dim, projection_dim)\n",
    "        self.do   = nn.Dropout(dropout)\n",
    "        self.ln   = nn.LayerNorm(projection_dim)\n",
    "    def forward(self, x):\n",
    "        h = self.proj(x)\n",
    "        x = self.gelu(h)\n",
    "        x = self.fc(x)\n",
    "        x = self.do(x)\n",
    "        return self.ln(x + h)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# UNI2 image encoder (timm) – put into train mode for fine‑tuning\n",
    "# -----------------------------------------------------------------------------\n",
    "uni2_cfg = {\n",
    "    'model_name':'vit_giant_patch14_224','img_size':224,'patch_size':14,'depth':24,\n",
    "    'num_heads':24,'init_values':1e-5,'embed_dim':1536,'mlp_ratio':2.66667*2,\n",
    "    'num_classes':0,'no_embed_class':True,'mlp_layer':timm.layers.SwiGLUPacked,\n",
    "    'act_layer':torch.nn.SiLU,'reg_tokens':8,'dynamic_img_size':True,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "uni2 = timm.create_model(pretrained=False, **uni2_cfg)\n",
    "uni2.load_state_dict(torch.load(os.path.join(CFG.model_dir, \"pytorch_model.bin\"), map_location=\"cpu\"), strict=True)\n",
    "uni2 = uni2.to(device).train()\n",
    "\n",
    "prefix_tokens = getattr(uni2, \"num_prefix_tokens\", 9)\n",
    "level_idx_map = {\n",
    "    0: torch.tensor([119,120,135,136]),\n",
    "    1: torch.tensor([102,103,104,105,118,119,120,121,134,135,136,137,150,151,152,153]),\n",
    "}\n",
    "center_idx = level_idx_map[CFG.level].to(device)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Full BLEEP‑style model\n",
    "# -----------------------------------------------------------------------------\n",
    "class BLEEP_UNI2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_enc: nn.Module,\n",
    "        gene_dim: int,\n",
    "        morph_dim: int = CFG.morph_emb_dims,     # 1536\n",
    "        init_temp: float = CFG.temperature       # 1.0 → logit_scale = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # encoders + projection heads\n",
    "        self.image_encoder = img_enc\n",
    "        self.image_proj    = ProjectionHead(morph_dim)\n",
    "        self.gene_proj     = ProjectionHead(gene_dim)\n",
    "\n",
    "        # learnable logit-scale   (log(1/τ))\n",
    "        self.logit_scale = nn.Parameter(torch.log(torch.tensor(1.0 / init_temp)))\n",
    "\n",
    "        # register centre-token indices so they move with .to(device)\n",
    "        self.prefix_tokens = 9                           # CLS + 8 REG\n",
    "        self.register_buffer(\n",
    "            \"center_idx\",\n",
    "            torch.tensor([119, 120, 135, 136], dtype=torch.long),\n",
    "            persistent=False\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def _encode_image(self, imgs: torch.Tensor) -> torch.Tensor:\n",
    "        tok     = self.image_encoder.forward_features(imgs)        # (B, 265, 1536)\n",
    "        spatial = tok[:, self.prefix_tokens:, :]                  # drop prefixes\n",
    "        center  = spatial.index_select(1, self.center_idx).mean(1)\n",
    "        return self.image_proj(center)                            # (B,256)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def forward(self, imgs: torch.Tensor, genes: torch.Tensor) -> torch.Tensor:\n",
    "        #  embeddings (L2-normalised)\n",
    "        img_vec  = F.normalize(self._encode_image(imgs),  dim=-1)  # (B,256)\n",
    "        gene_vec = F.normalize(self.gene_proj(genes),     dim=-1)  # (B,256)\n",
    "\n",
    "        # contrastive logits with learnable temperature\n",
    "        scale  = self.logit_scale.exp()                            # scalar > 0\n",
    "        logits = scale * (gene_vec @ img_vec.T)                    # (B,B)\n",
    "\n",
    "        # smoothed intra-modal targets (no gradients needed)\n",
    "        with torch.no_grad():\n",
    "            sim_img  = scale * (img_vec  @ img_vec.T)\n",
    "            sim_gene = scale * (gene_vec @ gene_vec.T)\n",
    "            targets  = F.softmax(0.5 * (sim_img + sim_gene), dim=-1)  # (B,B)\n",
    "\n",
    "        # cross-entropy, symmetrised\n",
    "        loss_gene = F.cross_entropy(logits,   targets,   reduction='none')\n",
    "        loss_img  = F.cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        return 0.5 * (loss_gene + loss_img).mean()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# DataLoader\n",
    "# -----------------------------------------------------------------------------\n",
    "loader = DataLoader(\n",
    "    CellPatchDataset(slide, cell_df, gene_emb, transform, scale_factor, CFG.patch_size),\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=CFG.num_workers,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4,\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Optimiser & training loop\n",
    "# -----------------------------------------------------------------------------\n",
    "model = BLEEP_UNI2(uni2, gene_emb.shape[1]).to(device)\n",
    "# opt   = torch.optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "scaler = GradScaler()                         \n",
    "opt = torch.optim.AdamW(model.parameters(),\n",
    "                        lr=1e-3,           # ↑ ten-fold\n",
    "                        weight_decay=CFG.weight_decay)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                opt, T_max=CFG.epochs, eta_min=1e-5)\n",
    "os.makedirs(CFG.ckpt_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "best = float(\"inf\")\n",
    "def weights_have_nan(m):\n",
    "    return any(torch.isnan(p).any() for p in m.parameters())\n",
    "\n",
    "# ─────────────────────────────────── updated inner loop ───────────────────────────────────\n",
    "for epoch in range(1, CFG.epochs + 1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    t0 = time.time()\n",
    "\n",
    "    prog = tqdm(loader, desc=f\"E{epoch}/{CFG.epochs}\", unit=\"batch\")\n",
    "    for step, batch in enumerate(prog, 1):\n",
    "        imgs  = batch[\"image\"].to(device, non_blocking=True)\n",
    "        genes = batch[\"gene\"].to(device, non_blocking=True)\n",
    "\n",
    "        # ── 1. forward pass ──────────────────────────────────────────\n",
    "        #    (only the ViT backbone in AMP / FP16)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            tok = model.image_encoder.forward_features(imgs)     # FP16\n",
    "\n",
    "        tok = tok.float()                                        # back to FP32\n",
    "        spatial = tok[:, model.prefix_tokens:, :]\n",
    "        center  = spatial.index_select(1, model.center_idx).mean(1)\n",
    "\n",
    "        img_vec  = F.normalize(model.image_proj(center), dim=-1) # FP32\n",
    "        gene_vec = F.normalize(model.gene_proj(genes),  dim=-1)  # FP32\n",
    "\n",
    "        scale  = model.logit_scale.exp()\n",
    "        logits = scale * (gene_vec @ img_vec.T)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sim_i = scale * (img_vec  @ img_vec.T)\n",
    "            sim_g = scale * (gene_vec @ gene_vec.T)\n",
    "            targets = F.softmax(0.5 * (sim_i + sim_g), dim=-1)\n",
    "\n",
    "        loss_g = F.cross_entropy(logits,   targets,   reduction='none')\n",
    "        loss_i = F.cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        base_loss = 0.5 * (loss_g + loss_i).mean()\n",
    "\n",
    "        if CFG.use_L1_reg:\n",
    "            l1_pen = torch.zeros([], device=imgs.device)\n",
    "            for n, p in model.named_parameters():\n",
    "                if p.requires_grad and (\"image_proj\" in n or \"gene_proj\" in n):\n",
    "                    l1_pen += p.abs().sum()\n",
    "            total_loss = base_loss + CFG.l1_lambda * l1_pen\n",
    "        else:\n",
    "            total_loss = base_loss\n",
    "\n",
    "        # ── 2. clamp temperature & backward ───────────────────────\n",
    "        model.logit_scale.data.clamp_(min=-10.0, max=4.6)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        if epoch >= 4:\n",
    "            scheduler.step()\n",
    "\n",
    "        # ── 3. diagnostics & NaN guard ────────────────────────────\n",
    "        with torch.no_grad():\n",
    "            tau = 1.0 / model.logit_scale.exp()\n",
    "            grad_norm = torch.norm(\n",
    "                torch.stack([p.grad.norm() for p in model.parameters()\n",
    "                             if p.grad is not None])\n",
    "            )\n",
    "            if torch.isnan(grad_norm) or torch.isnan(total_loss):\n",
    "                raise RuntimeError(f\"NaN detected at epoch {epoch}, step {step}\")\n",
    "\n",
    "        running += total_loss.item()\n",
    "        prog.set_postfix(loss=running / step,\n",
    "                         lr=scheduler.get_last_lr()[0])\n",
    "\n",
    "    avg = running / len(loader)\n",
    "    print(f\"Epoch {epoch}: avg loss {avg:.4f} | τ={tau:.3e} | grad‖={grad_norm:.2f} \"\n",
    "          f\"| time {(time.time()-t0)/60:.1f} min\")\n",
    "\n",
    "    # --- checkpointing (unchanged) ---\n",
    "    ckpt = {\n",
    "        \"epoch\":  epoch,\n",
    "        \"model\":  model.state_dict(),\n",
    "        \"opt\":    opt.state_dict(),\n",
    "        \"loss\":   avg,\n",
    "    }\n",
    "    ep_path = os.path.join(CFG.ckpt_dir, f\"epoch_{epoch:03d}.pth\")\n",
    "    torch.save(ckpt, ep_path)\n",
    "\n",
    "    if avg < best:\n",
    "        best = avg\n",
    "        torch.save({\"epoch\": epoch, \"model\": model.state_dict()},\n",
    "                   os.path.join(CFG.ckpt_dir, \"best.pth\"))\n",
    "        print(\"✓ new best saved\")\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "print(\"Training complete. Best loss:\", best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463829fb-b9c9-4acd-9b68-962d7fa59c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phacosta (py3.10.12)",
   "language": "python",
   "name": "phacosta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
